{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dnn_mnist.ipynb의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ByeongGil-Jung/2019-Deep-Learning/blob/project_1/project_1/dnn_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "eMrOM1B838Eg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "174aeabe-58c6-45e3-b857-426aae002ba5"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "DNN to classify MNIST handwritten digits\n",
        "'''\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-374d72521340>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "osqEHrEK38Em",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "outputId": "2ced5e3b-27a0-4c3e-b637-affae02deca1"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task #1 & Task #2\n",
        "\"\"\"\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "n_epochs = 20\n",
        "batch_size = 100\n",
        "display_step = 1\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 256 # 1st layer number of features\n",
        "n_hidden_2 = 256 # 2nd layer number of features\n",
        "n_input = 784 # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
        "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
        "\n",
        "\n",
        "# Create model\n",
        "def multilayer_perceptron(x, weights, biases):\n",
        "    # Hidden layer\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    # Hidden layer\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    # Output layer with linear activation\n",
        "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "# Construct model\n",
        "pred = multilayer_perceptron(X, weights, biases)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
        "\n",
        "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # Training \n",
        "    for i in range(n_epochs):\n",
        "        total_loss = 0.\n",
        "        n_batches = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for j in range(n_batches):\n",
        "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
        "            # Compute average loss\n",
        "            total_loss += l\n",
        "        # Display logs per epoch step\n",
        "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "\n",
        "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
        "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "    \n",
        "    n_batches = int(mnist.test.num_examples/batch_size)\n",
        "    total_correct_preds = 0\n",
        "    \n",
        "    for i in range(n_batches):\n",
        "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
        "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
        "        total_correct_preds += accuracy_batch   \n",
        "    \n",
        "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-2-a704a38ab237>:51: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Average loss epoch 0: 153.05315231323243\n",
            "Average loss epoch 1: 34.54869186878204\n",
            "Average loss epoch 2: 21.049974619366907\n",
            "Average loss epoch 3: 14.835815763370338\n",
            "Average loss epoch 4: 10.706459432813254\n",
            "Average loss epoch 5: 8.107694595063792\n",
            "Average loss epoch 6: 6.216438831114785\n",
            "Average loss epoch 7: 4.786875377801772\n",
            "Average loss epoch 8: 3.7467409478243803\n",
            "Average loss epoch 9: 2.915628507758595\n",
            "Average loss epoch 10: 2.3468205263353075\n",
            "Average loss epoch 11: 1.8642536700909957\n",
            "Average loss epoch 12: 1.4926211005556866\n",
            "Average loss epoch 13: 1.145184236779962\n",
            "Average loss epoch 14: 0.9983437227733928\n",
            "Average loss epoch 15: 0.7785243825667583\n",
            "Average loss epoch 16: 0.6804330382288742\n",
            "Average loss epoch 17: 0.5420613157334816\n",
            "Average loss epoch 18: 0.5045355988459506\n",
            "Average loss epoch 19: 0.44175329354012155\n",
            "Optimization Finished!\n",
            "Accuracy 0.9506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bBcFSYhv9wiA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task #3\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "task3_data_dict = {\"Features\": np.array([256, 128, 64]),\n",
        "                   \"Accuracy\": np.zeros(3),\n",
        "                   \"Num of Hidden Layers\": np.zeros(3),\n",
        "                   \"Num of Learning Params\": np.zeros(3),\n",
        "                   \"Avg Loss (Last Epoch)\": np.zeros(3),\n",
        "                   \"Elapsed Time\": np.zeros(3)}\n",
        "\n",
        "task3_df = pd.DataFrame.from_dict(task3_data_dict)\n",
        "task3_df.index = task3_df[\"Features\"]\n",
        "del task3_df[\"Features\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lnUrGE6c4dQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "160d0d97-641c-410b-dfa5-00e5ee1bcf66"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task #3 - 1 : DNN with two hidden layers, each layer has 256 units (as in the given code)\n",
        "\"\"\"\n",
        "\n",
        "# Pandas Params\n",
        "row_index = 256\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "n_epochs = 20\n",
        "batch_size = 100\n",
        "display_step = 1\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 256 # 1st layer number of features\n",
        "n_hidden_2 = 256 # 2nd layer number of features\n",
        "n_input = 784 # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "task3_df.loc[row_index, \"Num of Hidden Layers\"] = 2\n",
        "task3_df.loc[row_index, \"Num of Learning Params\"] = n_input * n_hidden_1 * n_hidden_2\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
        "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
        "\n",
        "\n",
        "# Create model\n",
        "def multilayer_perceptron(x, weights, biases):\n",
        "    # Hidden layer\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    # Hidden layer\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    # Output layer with linear activation\n",
        "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "# Construct model\n",
        "pred = multilayer_perceptron(X, weights, biases)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
        "\n",
        "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    start = time.time()\n",
        "    # Training \n",
        "    for i in range(n_epochs):\n",
        "        total_loss = 0.\n",
        "        n_batches = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for j in range(n_batches):\n",
        "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
        "            # Compute average loss\n",
        "            total_loss += l\n",
        "        # Display logs per epoch step\n",
        "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
        "        \n",
        "        if (j == (n_batches - 1)):\n",
        "            task3_df.loc[row_index, \"Avg Loss (Last Epoch)\"] = total_loss / n_batches\n",
        "            \n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    task3_df.loc[row_index, \"Elapsed Time\"] = end - start\n",
        "\n",
        "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
        "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "    \n",
        "    n_batches = int(mnist.test.num_examples/batch_size)\n",
        "    total_correct_preds = 0\n",
        "    \n",
        "    for i in range(n_batches):\n",
        "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
        "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
        "        total_correct_preds += accuracy_batch   \n",
        "    \n",
        "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
        "    \n",
        "    task3_df.loc[row_index, \"Accuracy\"] = total_correct_preds / mnist.test.num_examples"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss epoch 0: 161.75211776733397\n",
            "Average loss epoch 1: 32.77292487231168\n",
            "Average loss epoch 2: 19.97018756527792\n",
            "Average loss epoch 3: 13.901552612944084\n",
            "Average loss epoch 4: 10.347553300139579\n",
            "Average loss epoch 5: 7.60185929791242\n",
            "Average loss epoch 6: 5.813232704766897\n",
            "Average loss epoch 7: 4.433505963434676\n",
            "Average loss epoch 8: 3.499914564762364\n",
            "Average loss epoch 9: 2.7621702762940132\n",
            "Average loss epoch 10: 2.135802597118354\n",
            "Average loss epoch 11: 1.6886929274170297\n",
            "Average loss epoch 12: 1.3922709836354998\n",
            "Average loss epoch 13: 1.0771479788235583\n",
            "Average loss epoch 14: 0.9805187265026399\n",
            "Average loss epoch 15: 0.7792720241309984\n",
            "Average loss epoch 16: 0.620552207996282\n",
            "Average loss epoch 17: 0.5524935171091884\n",
            "Average loss epoch 18: 0.4588337336576273\n",
            "Average loss epoch 19: 0.3893880875661087\n",
            "Optimization Finished!\n",
            "Accuracy 0.948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y_XbqwgoKUKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "3723b71f-0ef9-4db2-fe5e-b534a3d26e18"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task #3 - 2 : DNN with three hidden layers, each layer has 128 units\n",
        "\"\"\"\n",
        "\n",
        "# Pandas Params\n",
        "row_index = 128\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "n_epochs = 20\n",
        "batch_size = 100\n",
        "display_step = 1\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 128 # 1st layer number of features\n",
        "n_hidden_2 = 128 # 2nd layer number of features\n",
        "n_hidden_3 = 128 # 3rd layer number of features\n",
        "n_input = 784 # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "task3_df.loc[row_index, \"Num of Hidden Layers\"] = 3\n",
        "task3_df.loc[row_index, \"Num of Learning Params\"] = n_input * n_hidden_1 * n_hidden_2 * n_hidden_3\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
        "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
        "\n",
        "\n",
        "# Create model\n",
        "def multilayer_perceptron(x, weights, biases):\n",
        "    # Hidden layer\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    # Hidden layer\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    # Hidden layer\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    layer_3 = tf.nn.relu(layer_3)\n",
        "    # Output layer with linear activation\n",
        "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "# Construct model\n",
        "pred = multilayer_perceptron(X, weights, biases)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
        "\n",
        "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    start = time.time()\n",
        "    # Training \n",
        "    for i in range(n_epochs):\n",
        "        total_loss = 0.\n",
        "        n_batches = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for j in range(n_batches):\n",
        "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
        "            # Compute average loss\n",
        "            total_loss += l\n",
        "        # Display logs per epoch step\n",
        "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
        "        \n",
        "        if (j == (n_batches - 1)):\n",
        "            task3_df.loc[row_index, \"Avg Loss (Last Epoch)\"] = total_loss / n_batches\n",
        "            \n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    task3_df.loc[row_index, \"Elapsed Time\"] = end - start\n",
        "\n",
        "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
        "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "    \n",
        "    n_batches = int(mnist.test.num_examples/batch_size)\n",
        "    total_correct_preds = 0\n",
        "    \n",
        "    for i in range(n_batches):\n",
        "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
        "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
        "        total_correct_preds += accuracy_batch   \n",
        "    \n",
        "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
        "    \n",
        "    task3_df.loc[row_index, \"Accuracy\"] = total_correct_preds / mnist.test.num_examples"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss epoch 0: 605.0163947642933\n",
            "Average loss epoch 1: 119.64037280342795\n",
            "Average loss epoch 2: 71.04745685490695\n",
            "Average loss epoch 3: 49.49430141860788\n",
            "Average loss epoch 4: 36.88555232893337\n",
            "Average loss epoch 5: 28.535161086971108\n",
            "Average loss epoch 6: 22.69612650979649\n",
            "Average loss epoch 7: 18.15500311356553\n",
            "Average loss epoch 8: 14.703215063823896\n",
            "Average loss epoch 9: 12.063685850633348\n",
            "Average loss epoch 10: 10.234946236584047\n",
            "Average loss epoch 11: 8.492248942470095\n",
            "Average loss epoch 12: 7.089848179589\n",
            "Average loss epoch 13: 5.984559438397651\n",
            "Average loss epoch 14: 5.095772584290289\n",
            "Average loss epoch 15: 4.432371449888431\n",
            "Average loss epoch 16: 3.8694188242309777\n",
            "Average loss epoch 17: 3.2842042136855194\n",
            "Average loss epoch 18: 2.8399111003662147\n",
            "Average loss epoch 19: 2.3456248241145605\n",
            "Optimization Finished!\n",
            "Accuracy 0.9331\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BcFdVlTJL8Re",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "be45691f-9a21-423e-9d7a-0a78cda6eb92"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task #3 - 3 : DNN with four hidden layers, each layer has 64 units\n",
        "\"\"\"\n",
        "\n",
        "# Pandas Params\n",
        "row_index = 64\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "n_epochs = 20\n",
        "batch_size = 100\n",
        "display_step = 1\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 64 # 1st layer number of features\n",
        "n_hidden_2 = 64 # 2nd layer number of features\n",
        "n_hidden_3 = 64 # 3rd layer number of features\n",
        "n_hidden_4 = 64 # 4th layer number of features\n",
        "n_input = 784 # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "task3_df.loc[row_index, \"Num of Hidden Layers\"] = 4\n",
        "task3_df.loc[row_index, \"Num of Learning Params\"] = n_input * n_hidden_1 * n_hidden_2 * n_hidden_3 * n_hidden_4\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
        "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
        "\n",
        "\n",
        "# Create model\n",
        "def multilayer_perceptron(x, weights, biases):\n",
        "    # Hidden layer\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    # Hidden layer\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    # Hidden layer\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    layer_3 = tf.nn.relu(layer_3)\n",
        "    # Hidden layer\n",
        "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
        "    layer_4 = tf.nn.relu(layer_4)\n",
        "    # Output layer with linear activation\n",
        "    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
        "    return out_layer\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
        "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
        "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "# Construct model\n",
        "pred = multilayer_perceptron(X, weights, biases)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
        "\n",
        "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "\n",
        "\n",
        "# Launch the graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    start = time.time()\n",
        "    # Training \n",
        "    for i in range(n_epochs):\n",
        "        total_loss = 0.\n",
        "        n_batches = int(mnist.train.num_examples/batch_size)\n",
        "        # Loop over all batches\n",
        "        for j in range(n_batches):\n",
        "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
        "            # Compute average loss\n",
        "            total_loss += l\n",
        "        # Display logs per epoch step\n",
        "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
        "        \n",
        "        if (j == (n_batches - 1)):\n",
        "            task3_df.loc[row_index, \"Avg Loss (Last Epoch)\"] = total_loss / n_batches\n",
        "            \n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    task3_df.loc[row_index, \"Elapsed Time\"] = end - start\n",
        "\n",
        "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
        "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "    \n",
        "    n_batches = int(mnist.test.num_examples/batch_size)\n",
        "    total_correct_preds = 0\n",
        "    \n",
        "    for i in range(n_batches):\n",
        "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
        "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
        "        total_correct_preds += accuracy_batch   \n",
        "    \n",
        "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
        "    \n",
        "    task3_df.loc[row_index, \"Accuracy\"] = total_correct_preds / mnist.test.num_examples"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss epoch 0: 1240.6651515059039\n",
            "Average loss epoch 1: 218.95880047191272\n",
            "Average loss epoch 2: 124.72761780478737\n",
            "Average loss epoch 3: 84.02891473770141\n",
            "Average loss epoch 4: 62.18773619044911\n",
            "Average loss epoch 5: 48.38673423420299\n",
            "Average loss epoch 6: 38.263797568407924\n",
            "Average loss epoch 7: 31.384788921529598\n",
            "Average loss epoch 8: 26.305756939866328\n",
            "Average loss epoch 9: 22.26959834142165\n",
            "Average loss epoch 10: 19.12652452924035\n",
            "Average loss epoch 11: 16.69009042208845\n",
            "Average loss epoch 12: 14.66473642912778\n",
            "Average loss epoch 13: 12.956089394526048\n",
            "Average loss epoch 14: 11.60253980926492\n",
            "Average loss epoch 15: 10.41098059696738\n",
            "Average loss epoch 16: 9.322097208946943\n",
            "Average loss epoch 17: 8.360155446509069\n",
            "Average loss epoch 18: 7.6526207690618255\n",
            "Average loss epoch 19: 7.000530377075347\n",
            "Optimization Finished!\n",
            "Accuracy 0.9054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S6lUG0RUM2ee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "7609097d-5a7c-4059-e105-9cbaaaa45fed"
      },
      "cell_type": "code",
      "source": [
        "task3_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Avg Loss (Last Epoch)</th>\n",
              "      <th>Elapsed Time</th>\n",
              "      <th>Num of Hidden Layers</th>\n",
              "      <th>Num of Learning Params</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Features</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>0.9480</td>\n",
              "      <td>0.389388</td>\n",
              "      <td>27.174656</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.138022e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>0.9331</td>\n",
              "      <td>2.345625</td>\n",
              "      <td>28.431317</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.644167e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>0.9054</td>\n",
              "      <td>7.000530</td>\n",
              "      <td>30.036170</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.315334e+10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Accuracy  Avg Loss (Last Epoch)  Elapsed Time  Num of Hidden Layers  \\\n",
              "Features                                                                        \n",
              "256         0.9480               0.389388     27.174656                   2.0   \n",
              "128         0.9331               2.345625     28.431317                   3.0   \n",
              "64          0.9054               7.000530     30.036170                   4.0   \n",
              "\n",
              "          Num of Learning Params  \n",
              "Features                          \n",
              "256                 5.138022e+07  \n",
              "128                 1.644167e+09  \n",
              "64                  1.315334e+10  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "KTW98RN9PKiH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aba5c792-7bd3-42f3-bd20-0a60468c0a87"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Feature 의 갯수가 적고, Layer 가 깊어질 수록 \n",
        "\"\"\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFeature 의 갯수가 적고, Layer 가 깊어질 수록 \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "TF4XI_AmiUWv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Task #4\n",
        "\"\"\"\n",
        "model_data_dict = dict()\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 512 # 1st layer number of features\n",
        "n_hidden_2 = 256 # 2nd layer number of features\n",
        "n_input = 784 # MNIST data input (img shape: 28*28)\n",
        "n_classes = 10 # MNIST total classes (0-9 digits)\n",
        "\n",
        "# 784 -> 512 -> 256 -> 10\n",
        "class MyModel_1(object):\n",
        "    \n",
        "    def __init__(self, sess, learning_rate, batch_size):\n",
        "        self.sess = sess\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_1_size = 512\n",
        "        self.hidden_2_size = 256\n",
        "        \n",
        "        self.X = tf.placeholder(tf.float32, shape=[self.batch_size, n_input])\n",
        "        self.Y = tf.placeholder(tf.float32, shape=[self.batch_size, n_classes])\n",
        "        \n",
        "        self.pred = None\n",
        "        self.loss = None\n",
        "        self.optimizer = None\n",
        "        \n",
        "        self.correct_preds = None\n",
        "        self.accuracy = None\n",
        "        \n",
        "    def build_model(self):\n",
        "        with tf.variable_scope(\"Layer1\"):\n",
        "            W1 = tf.Variable(tf.random_normal([n_input, self.hidden_1_size]))\n",
        "            b1 = tf.Variable(tf.random_normal([self.hidden_1_size]))\n",
        "            layer1 = tf.add(tf.matmul(self.X, W1), b1)\n",
        "            layer1 = tf.nn.relu(layer1)\n",
        "            \n",
        "        with tf.variable_scope(\"Layer2\"):\n",
        "            W2 = tf.Variable(tf.random_normal([self.hidden_1_size, self.hidden_2_size]))\n",
        "            b2 = tf.Variable(tf.random_normal([self.hidden_2_size]))\n",
        "            layer2 = tf.add(tf.matmul(layer1, W2), b2)\n",
        "            layer2 = tf.nn.relu(layer2)\n",
        "        \n",
        "        with tf.variable_scope(\"Layer3\"):\n",
        "            W3 = tf.Variable(tf.random_normal([self.hidden_2_size, n_classes]))\n",
        "            b3 = tf.Variable(tf.random_normal([n_classes]))\n",
        "            layer3 = tf.add(tf.matmul(layer2, W3), b3)\n",
        "        \n",
        "        self.pred = layer3\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.Y))\n",
        "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "        \n",
        "        self.correct_preds = tf.equal(tf.argmax(self.pred, axis=1), tf.argmax(self.Y, axis=1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_preds, tf.float32))\n",
        "    \n",
        "    def train(self, x_batch, y_batch):\n",
        "        return self.sess.run(fetches=[self.loss, self.optimizer], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
        "    \n",
        "    def get_accuracy(self, x_batch, y_batch):\n",
        "        return self.sess.run(fetches=[self.accuracy], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
        "\n",
        "\n",
        "# 784 -> 784 -> 512 -> 10\n",
        "class MyModel_2(object):\n",
        "    \n",
        "    def __init__(self, sess, learning_rate, batch_size):\n",
        "        self.sess = sess\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_1_size = 784\n",
        "        self.hidden_2_size = 512\n",
        "        \n",
        "        self.X = tf.placeholder(tf.float32, shape=[self.batch_size, n_input])\n",
        "        self.Y = tf.placeholder(tf.float32, shape=[self.batch_size, n_classes])\n",
        "        \n",
        "        self.pred = None\n",
        "        self.loss = None\n",
        "        self.optimizer = None\n",
        "        \n",
        "        self.correct_preds = None\n",
        "        self.accuracy = None\n",
        "        \n",
        "    def build_model(self):\n",
        "        with tf.variable_scope(\"Layer1\"):\n",
        "            W1 = tf.Variable(tf.random_normal([n_input, self.hidden_1_size]))\n",
        "            b1 = tf.Variable(tf.random_normal([self.hidden_1_size]))\n",
        "            layer1 = tf.add(tf.matmul(self.X, W1), b1)\n",
        "            layer1 = tf.nn.relu(layer1)\n",
        "            \n",
        "        with tf.variable_scope(\"Layer2\"):\n",
        "            W2 = tf.Variable(tf.random_normal([self.hidden_1_size, self.hidden_2_size]))\n",
        "            b2 = tf.Variable(tf.random_normal([self.hidden_2_size]))\n",
        "            layer2 = tf.add(tf.matmul(layer1, W2), b2)\n",
        "            layer2 = tf.nn.relu(layer2)\n",
        "        \n",
        "        with tf.variable_scope(\"Layer3\"):\n",
        "            W3 = tf.Variable(tf.random_normal([self.hidden_2_size, n_classes]))\n",
        "            b3 = tf.Variable(tf.random_normal([n_classes]))\n",
        "            layer3 = tf.add(tf.matmul(layer2, W3), b3)\n",
        "        \n",
        "        self.pred = layer3\n",
        "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.Y))\n",
        "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
        "        \n",
        "        self.correct_preds = tf.equal(tf.argmax(self.pred, axis=1), tf.argmax(self.Y, axis=1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_preds, tf.float32))\n",
        "        \n",
        "    def train(self, x_batch, y_batch):\n",
        "        return self.sess.run(fetches=[self.loss, self.optimizer], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
        "    \n",
        "    def get_accuracy(self, x_batch, y_batch):\n",
        "        return self.sess.run(fetches=[self.accuracy], feed_dict={self.X: x_batch, self.Y: y_batch})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPylf-EVPobI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1119
        },
        "outputId": "224fea92-2b6b-4291-e624-454bdc889d80"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Model 1 (MyModel_1)\n",
        "\"\"\"\n",
        "\n",
        "# Parameters\n",
        "n_epochs_list = [10, 20, 40, 80, 160, 320]\n",
        "batch_size_list = [100, 200]\n",
        "learning_rate_list = [1e-1, 1e-2, 1e-3]\n",
        "display_step = 1\n",
        "\n",
        "# n_epochs = 10\n",
        "# batch_size = 100\n",
        "# learning_rate = 1e-3\n",
        "\n",
        "\n",
        "# Launch the graph\n",
        "model_1_dict = dict()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    #Training\n",
        "    n_epochs_dict = dict()\n",
        "    for n_epochs in n_epochs_list:\n",
        "        batch_size_dict = dict()\n",
        "        \n",
        "        for batch_size in batch_size_list:\n",
        "            learning_rate_dict = dict()\n",
        "            \n",
        "            for learning_rate in learning_rate_list:\n",
        "                accuracy_dict = dict()\n",
        "                \n",
        "                model = MyModel_1(sess=sess, learning_rate=learning_rate, batch_size=batch_size)\n",
        "                model.build_model()\n",
        "                \n",
        "                # Start\n",
        "                sess.run(tf.global_variables_initializer())\n",
        "                \n",
        "                print(\"n_epochs : {}, batch_size : {}, learning_rate : {}\".format(n_epochs, batch_size, learning_rate))\n",
        "                \n",
        "                for i in range(n_epochs):\n",
        "                    total_loss = 0.\n",
        "                    n_batches = int(mnist.train.num_examples/batch_size)\n",
        "                    # Loop over all batches\n",
        "                    for j in range(n_batches):\n",
        "                        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
        "                        # Run optimization op (backprop) and cost op (to get loss value)\n",
        "                        l, _ = model.train(x_batch=X_batch, y_batch=Y_batch)\n",
        "                        # Compute average loss\n",
        "                        total_loss += l\n",
        "                    # Display logs per epoch step\n",
        "                    # print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
        "\n",
        "                # print(\"Optimization Finished!\")\n",
        "\n",
        "                n_batches = int(mnist.test.num_examples/batch_size)\n",
        "                total_correct_preds = 0\n",
        "\n",
        "                for i in range(n_batches):\n",
        "                    X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
        "                    accuracy_batch = model.get_accuracy(x_batch=X_batch, y_batch=Y_batch)\n",
        "                    total_correct_preds += accuracy_batch[0]\n",
        "\n",
        "                accuracy = total_correct_preds / n_batches\n",
        "                print('> Accuracy : {0}'.format(accuracy))\n",
        "                \n",
        "                learning_rate_dict[learning_rate] = accuracy\n",
        "            \n",
        "            batch_size_dict[batch_size] = learning_rate_dict\n",
        "            \n",
        "        n_epochs_dict[n_epochs] = batch_size_dict\n",
        "    \n",
        "    model_1_dict[\"model_1\"] = n_epochs_dict"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_epochs : 10, batch_size : 100, learning_rate : 0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ba5b2b907170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[0;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                         \u001b[0;31m# Compute average loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-54f05584194f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_batch, y_batch)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4t7PxbBKzmHy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9bde61e9-6edd-472e-e88e-ed1d5be13f3f"
      },
      "cell_type": "code",
      "source": [
        "model_1_dict"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_1': {10: {100: {0.001: 0.9501999980211258}}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "ykxWV2-XKYbX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}