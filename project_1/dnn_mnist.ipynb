{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Project_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task #1 & Task #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "id": "eMrOM1B838Eg",
    "outputId": "174aeabe-58c6-45e3-b857-426aae002ba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-374d72521340>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DNN to classify MNIST handwritten digits\n",
    "'''\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "colab_type": "code",
    "id": "osqEHrEK38Em",
    "outputId": "2ced5e3b-27a0-4c3e-b637-affae02deca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-a704a38ab237>:51: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Average loss epoch 0: 168.3850292119113\n",
      "Average loss epoch 1: 33.46105378627777\n",
      "Average loss epoch 2: 21.040984600240535\n",
      "Average loss epoch 3: 14.615540624490961\n",
      "Average loss epoch 4: 10.757293025444854\n",
      "Average loss epoch 5: 8.032111677757719\n",
      "Average loss epoch 6: 6.111178395861966\n",
      "Average loss epoch 7: 4.594843542595867\n",
      "Average loss epoch 8: 3.5686115723803864\n",
      "Average loss epoch 9: 2.8478462347794475\n",
      "Average loss epoch 10: 2.2000356414038875\n",
      "Average loss epoch 11: 1.647046978118015\n",
      "Average loss epoch 12: 1.3968338611474465\n",
      "Average loss epoch 13: 1.068735796603706\n",
      "Average loss epoch 14: 0.9721019016755769\n",
      "Average loss epoch 15: 0.6981170082559724\n",
      "Average loss epoch 16: 0.6100249284353196\n",
      "Average loss epoch 17: 0.5453274828418785\n",
      "Average loss epoch 18: 0.44694830609983427\n",
      "Average loss epoch 19: 0.38534472721827434\n",
      "Optimization Finished!\n",
      "Accuracy 0.9448\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task #1 & Task #2\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training \n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        n_batches = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            # Compute average loss\n",
    "            total_loss += l\n",
    "        # Display logs per epoch step\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        total_correct_preds += accuracy_batch   \n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBcFSYhv9wiA"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task #3\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "task3_data_dict = {\"Features\": np.array([256, 128, 64]),\n",
    "                   \"Accuracy\": np.zeros(3),\n",
    "                   \"Num of Hidden Layers\": np.zeros(3),\n",
    "                   \"Num of Learning Params\": np.zeros(3),\n",
    "                   \"Avg Loss (Last Epoch)\": np.zeros(3),\n",
    "                   \"Elapsed Time\": np.zeros(3)}\n",
    "\n",
    "task3_df = pd.DataFrame.from_dict(task3_data_dict)\n",
    "task3_df.index = task3_df[\"Features\"]\n",
    "del task3_df[\"Features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "lnUrGE6c4dQr",
    "outputId": "160d0d97-641c-410b-dfa5-00e5ee1bcf66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 150.2354018315402\n",
      "Average loss epoch 1: 34.373104598738934\n",
      "Average loss epoch 2: 21.107679878960955\n",
      "Average loss epoch 3: 14.4672446757555\n",
      "Average loss epoch 4: 10.437434271330183\n",
      "Average loss epoch 5: 7.768763152445886\n",
      "Average loss epoch 6: 5.7996919524475485\n",
      "Average loss epoch 7: 4.447629981860639\n",
      "Average loss epoch 8: 3.405731862537151\n",
      "Average loss epoch 9: 2.637055498776012\n",
      "Average loss epoch 10: 2.0290278591944397\n",
      "Average loss epoch 11: 1.5967412873049553\n",
      "Average loss epoch 12: 1.2134866588001714\n",
      "Average loss epoch 13: 0.9021374697865949\n",
      "Average loss epoch 14: 0.794327641993073\n",
      "Average loss epoch 15: 0.6079673807008411\n",
      "Average loss epoch 16: 0.48518941233932805\n",
      "Average loss epoch 17: 0.4086556032073026\n",
      "Average loss epoch 18: 0.3734318659139086\n",
      "Average loss epoch 19: 0.3137534890368079\n",
      "Optimization Finished!\n",
      "Accuracy 0.9448\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task #3 - 1 : DNN with two hidden layers, each layer has 256 units (as in the given code)\n",
    "\"\"\"\n",
    "\n",
    "# Pandas Params\n",
    "row_index = 256\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "task3_df.loc[row_index, \"Num of Hidden Layers\"] = 2\n",
    "task3_df.loc[row_index, \"Num of Learning Params\"] = (n_hidden_1 + 1) + (n_hidden_2 + 1)  # 514\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    start = time.time()\n",
    "    # Training \n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        n_batches = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            # Compute average loss\n",
    "            total_loss += l\n",
    "        # Display logs per epoch step\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        \n",
    "        if (j == (n_batches - 1)):\n",
    "            task3_df.loc[row_index, \"Avg Loss (Last Epoch)\"] = total_loss / n_batches\n",
    "            \n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    task3_df.loc[row_index, \"Elapsed Time\"] = end - start\n",
    "\n",
    "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        total_correct_preds += accuracy_batch   \n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "    \n",
    "    task3_df.loc[row_index, \"Accuracy\"] = total_correct_preds / mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "Y_XbqwgoKUKA",
    "outputId": "3723b71f-0ef9-4db2-fe5e-b534a3d26e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 679.0628434753418\n",
      "Average loss epoch 1: 119.2363397286155\n",
      "Average loss epoch 2: 69.66267197869041\n",
      "Average loss epoch 3: 47.29920308156447\n",
      "Average loss epoch 4: 34.544994585297324\n",
      "Average loss epoch 5: 26.66980153994127\n",
      "Average loss epoch 6: 20.69554401573937\n",
      "Average loss epoch 7: 16.906616189333146\n",
      "Average loss epoch 8: 13.738047696966651\n",
      "Average loss epoch 9: 11.22899821767694\n",
      "Average loss epoch 10: 9.378057014156472\n",
      "Average loss epoch 11: 7.802728737028362\n",
      "Average loss epoch 12: 6.579349261337718\n",
      "Average loss epoch 13: 5.547826656102472\n",
      "Average loss epoch 14: 4.585534736017153\n",
      "Average loss epoch 15: 3.959014181323207\n",
      "Average loss epoch 16: 3.4069790860712037\n",
      "Average loss epoch 17: 2.867893284688423\n",
      "Average loss epoch 18: 2.4810412897024503\n",
      "Average loss epoch 19: 2.2029979544820546\n",
      "Optimization Finished!\n",
      "Accuracy 0.9247\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task #3 - 2 : DNN with three hidden layers, each layer has 128 units\n",
    "\"\"\"\n",
    "\n",
    "# Pandas Params\n",
    "row_index = 128\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "n_hidden_2 = 128 # 2nd layer number of features\n",
    "n_hidden_3 = 128 # 3rd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "task3_df.loc[row_index, \"Num of Hidden Layers\"] = 3\n",
    "task3_df.loc[row_index, \"Num of Learning Params\"] = (n_hidden_1 + 1) + (n_hidden_2 + 1) + (n_hidden_3 + 1)  # 387\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Hidden layer\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    start = time.time()\n",
    "    # Training \n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        n_batches = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            # Compute average loss\n",
    "            total_loss += l\n",
    "        # Display logs per epoch step\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        \n",
    "        if (j == (n_batches - 1)):\n",
    "            task3_df.loc[row_index, \"Avg Loss (Last Epoch)\"] = total_loss / n_batches\n",
    "            \n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    task3_df.loc[row_index, \"Elapsed Time\"] = end - start\n",
    "\n",
    "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        total_correct_preds += accuracy_batch   \n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "    \n",
    "    task3_df.loc[row_index, \"Accuracy\"] = total_correct_preds / mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "BcFdVlTJL8Re",
    "outputId": "be45691f-9a21-423e-9d7a-0a78cda6eb92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1601.9591391546076\n",
      "Average loss epoch 1: 211.5347298223322\n",
      "Average loss epoch 2: 116.53942475752397\n",
      "Average loss epoch 3: 76.86575946287675\n",
      "Average loss epoch 4: 55.95346473520452\n",
      "Average loss epoch 5: 42.815124504782936\n",
      "Average loss epoch 6: 33.43493665088307\n",
      "Average loss epoch 7: 27.074209557446565\n",
      "Average loss epoch 8: 22.223648112687197\n",
      "Average loss epoch 9: 18.761376165693456\n",
      "Average loss epoch 10: 15.988538664471019\n",
      "Average loss epoch 11: 13.724543222947554\n",
      "Average loss epoch 12: 11.935046319439108\n",
      "Average loss epoch 13: 10.583587864258073\n",
      "Average loss epoch 14: 9.378487104793207\n",
      "Average loss epoch 15: 8.50182960300283\n",
      "Average loss epoch 16: 7.646449229229581\n",
      "Average loss epoch 17: 6.9291045269099145\n",
      "Average loss epoch 18: 6.321518710161961\n",
      "Average loss epoch 19: 5.7993867932872805\n",
      "Optimization Finished!\n",
      "Accuracy 0.8993\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task #3 - 3 : DNN with four hidden layers, each layer has 64 units\n",
    "\"\"\"\n",
    "\n",
    "# Pandas Params\n",
    "row_index = 64\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64 # 1st layer number of features\n",
    "n_hidden_2 = 64 # 2nd layer number of features\n",
    "n_hidden_3 = 64 # 3rd layer number of features\n",
    "n_hidden_4 = 64 # 4th layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "task3_df.loc[row_index, \"Num of Hidden Layers\"] = 4\n",
    "task3_df.loc[row_index, \"Num of Learning Params\"] = (n_hidden_1 + 1) + (n_hidden_2 + 1) + (n_hidden_3 + 1) + (n_hidden_4 + 1)  # 260\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Hidden layer\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "    # Hidden layer\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    start = time.time()\n",
    "    # Training \n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        n_batches = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            # Compute average loss\n",
    "            total_loss += l\n",
    "        # Display logs per epoch step\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        \n",
    "        if (j == (n_batches - 1)):\n",
    "            task3_df.loc[row_index, \"Avg Loss (Last Epoch)\"] = total_loss / n_batches\n",
    "            \n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    task3_df.loc[row_index, \"Elapsed Time\"] = end - start\n",
    "\n",
    "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        total_correct_preds += accuracy_batch   \n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "    \n",
    "    task3_df.loc[row_index, \"Accuracy\"] = total_correct_preds / mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "S6lUG0RUM2ee",
    "outputId": "7609097d-5a7c-4059-e105-9cbaaaa45fed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Avg Loss (Last Epoch)</th>\n",
       "      <th>Elapsed Time</th>\n",
       "      <th>Num of Hidden Layers</th>\n",
       "      <th>Num of Learning Params</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.9448</td>\n",
       "      <td>0.313753</td>\n",
       "      <td>12.678389</td>\n",
       "      <td>2.0</td>\n",
       "      <td>514.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.9247</td>\n",
       "      <td>2.202998</td>\n",
       "      <td>13.659215</td>\n",
       "      <td>3.0</td>\n",
       "      <td>387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.8993</td>\n",
       "      <td>5.799387</td>\n",
       "      <td>15.040273</td>\n",
       "      <td>4.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy  Avg Loss (Last Epoch)  Elapsed Time  Num of Hidden Layers  \\\n",
       "Features                                                                        \n",
       "256         0.9448               0.313753     12.678389                   2.0   \n",
       "128         0.9247               2.202998     13.659215                   3.0   \n",
       "64          0.8993               5.799387     15.040273                   4.0   \n",
       "\n",
       "          Num of Learning Params  \n",
       "Features                          \n",
       "256                        514.0  \n",
       "128                        387.0  \n",
       "64                         260.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task3_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can see that if hidden layers deepens, and the number of features in each layer decreases, the accuracy decreases. In this condition, the accuracy is proportional to the number of features and inversely proportional to the depth of layers. In addition, learning time is also increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's think about the reason. If increasing the number of features, it could find well characteristics that are able to distinguish data. That is, it is natural that the accuracy increases as the number of features increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, what is the correlation with the depth of the layer? The current model uses ReLU activation function in hidden layers. If the trained value is derived as negative number, ReLU activation function will replace the value with 0. That is, if there is a node that is derived to negative number previously, there exists a problem that the node will not be trained. So, in the situation that training with a simple data set like MNIST, if layers get deeper it will not train well. Therefore, in this situation, the accuracy is not proportional to the depth of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In Task #3, we found out that deeper the layers, it will not train well. Then to get the best result of training, we should set layers to shallow. So in this situation, fix the number of layers to 2.\n",
    " The number of models that are made for getting results is 3, and hyperparameter set is as follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[ Models ]__ \n",
    "\n",
    "Model_1:\n",
    "- hidden_1_size: 512\n",
    "- hidden_2_size: 256   \n",
    "  \n",
    "Model_2: \n",
    "- hidden_1_size: 784,  \n",
    "- hidden_2_size: 512  \n",
    "\n",
    "  \n",
    "Model_3:\n",
    "- hidden_1_size: 1024,  \n",
    "- hidden_2_size: 784  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[ Parameters ]__  \n",
    "- n_epochs_list = [10, 20, 40, 80, 160, 320]  \n",
    "- batch_size_list = [100, 200]  \n",
    "- learning_rate_list = [1e-1, 1e-2, 1e-3, 1e-4]  \n",
    "- display_step = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task #4\n",
    "\"\"\"\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TF4XI_AmiUWv"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models\n",
    "\"\"\"\n",
    "all_model_dict = dict()\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# 784 -> 512 -> 256 -> 10\n",
    "class MyModel_1(object):\n",
    "    \n",
    "    def __init__(self, sess, learning_rate, batch_size):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_1_size = 512\n",
    "        self.hidden_2_size = 256\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[self.batch_size, n_input])\n",
    "        self.Y = tf.placeholder(tf.float32, shape=[self.batch_size, n_classes])\n",
    "        \n",
    "        self.pred = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        self.correct_preds = None\n",
    "        self.accuracy = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(\"Layer1\"):\n",
    "            W1 = tf.Variable(tf.random_normal([n_input, self.hidden_1_size]))\n",
    "            b1 = tf.Variable(tf.random_normal([self.hidden_1_size]))\n",
    "            layer1 = tf.add(tf.matmul(self.X, W1), b1)\n",
    "            layer1 = tf.nn.relu(layer1)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer2\"):\n",
    "            W2 = tf.Variable(tf.random_normal([self.hidden_1_size, self.hidden_2_size]))\n",
    "            b2 = tf.Variable(tf.random_normal([self.hidden_2_size]))\n",
    "            layer2 = tf.add(tf.matmul(layer1, W2), b2)\n",
    "            layer2 = tf.nn.relu(layer2)\n",
    "        \n",
    "        with tf.variable_scope(\"Layer3\"):\n",
    "            W3 = tf.Variable(tf.random_normal([self.hidden_2_size, n_classes]))\n",
    "            b3 = tf.Variable(tf.random_normal([n_classes]))\n",
    "            layer3 = tf.add(tf.matmul(layer2, W3), b3)\n",
    "        \n",
    "        self.pred = layer3\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.Y))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.correct_preds = tf.equal(tf.argmax(self.pred, axis=1), tf.argmax(self.Y, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_preds, tf.float32))\n",
    "    \n",
    "    def train(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.loss, self.optimizer], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
    "    \n",
    "    def get_accuracy(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.accuracy], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
    "\n",
    "\n",
    "# 784 -> 784 -> 512 -> 10\n",
    "class MyModel_2(object):\n",
    "    \n",
    "    def __init__(self, sess, learning_rate, batch_size):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_1_size = 784\n",
    "        self.hidden_2_size = 512 \n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[self.batch_size, n_input])\n",
    "        self.Y = tf.placeholder(tf.float32, shape=[self.batch_size, n_classes])\n",
    "        \n",
    "        self.pred = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        self.correct_preds = None\n",
    "        self.accuracy = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(\"Layer1\"):\n",
    "            W1 = tf.Variable(tf.random_normal([n_input, self.hidden_1_size]))\n",
    "            b1 = tf.Variable(tf.random_normal([self.hidden_1_size]))\n",
    "            layer1 = tf.add(tf.matmul(self.X, W1), b1)\n",
    "            layer1 = tf.nn.relu(layer1)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer2\"):\n",
    "            W2 = tf.Variable(tf.random_normal([self.hidden_1_size, self.hidden_2_size]))\n",
    "            b2 = tf.Variable(tf.random_normal([self.hidden_2_size]))\n",
    "            layer2 = tf.add(tf.matmul(layer1, W2), b2)\n",
    "            layer2 = tf.nn.relu(layer2)\n",
    "        \n",
    "        with tf.variable_scope(\"Layer3\"):\n",
    "            W3 = tf.Variable(tf.random_normal([self.hidden_2_size, n_classes]))\n",
    "            b3 = tf.Variable(tf.random_normal([n_classes]))\n",
    "            layer3 = tf.add(tf.matmul(layer2, W3), b3)\n",
    "        \n",
    "        self.pred = layer3\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.Y))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.correct_preds = tf.equal(tf.argmax(self.pred, axis=1), tf.argmax(self.Y, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_preds, tf.float32))\n",
    "        \n",
    "    def train(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.loss, self.optimizer], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
    "    \n",
    "    def get_accuracy(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.accuracy], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
    "    \n",
    "# 784 -> 1024 -> 784 -> 10\n",
    "class MyModel_3(object):\n",
    "    \n",
    "    def __init__(self, sess, learning_rate, batch_size):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_1_size = 1024\n",
    "        self.hidden_2_size = 784\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[self.batch_size, n_input])\n",
    "        self.Y = tf.placeholder(tf.float32, shape=[self.batch_size, n_classes])\n",
    "        \n",
    "        self.pred = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        self.correct_preds = None\n",
    "        self.accuracy = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(\"Layer1\"):\n",
    "            W1 = tf.Variable(tf.random_normal([n_input, self.hidden_1_size]))\n",
    "            b1 = tf.Variable(tf.random_normal([self.hidden_1_size]))\n",
    "            layer1 = tf.add(tf.matmul(self.X, W1), b1)\n",
    "            layer1 = tf.nn.relu(layer1)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer2\"):\n",
    "            W2 = tf.Variable(tf.random_normal([self.hidden_1_size, self.hidden_2_size]))\n",
    "            b2 = tf.Variable(tf.random_normal([self.hidden_2_size]))\n",
    "            layer2 = tf.add(tf.matmul(layer1, W2), b2)\n",
    "            layer2 = tf.nn.relu(layer2)\n",
    "        \n",
    "        with tf.variable_scope(\"Layer3\"):\n",
    "            W3 = tf.Variable(tf.random_normal([self.hidden_2_size, n_classes]))\n",
    "            b3 = tf.Variable(tf.random_normal([n_classes]))\n",
    "            layer3 = tf.add(tf.matmul(layer2, W3), b3)\n",
    "        \n",
    "        self.pred = layer3\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.Y))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.correct_preds = tf.equal(tf.argmax(self.pred, axis=1), tf.argmax(self.Y, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_preds, tf.float32))\n",
    "        \n",
    "    def train(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.loss, self.optimizer], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
    "    \n",
    "    def get_accuracy(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.accuracy], feed_dict={self.X: x_batch, self.Y: y_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We will analyze the number of cases in all models and in all hyperparameters, and then will find out the highest accuracy. And after, we will also analyze how each parameter affects training and the what is the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1119
    },
    "colab_type": "code",
    "id": "CPylf-EVPobI",
    "outputId": "224fea92-2b6b-4291-e624-454bdc889d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.09579999979585409\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8378999978303909\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9503999990224838\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.8909000033140182\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11350000098347664\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9552999913692475\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.944399995803833\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.8801999974250794\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999975413085\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8800000023841857\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9554000043869019\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9155000030994416\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.19750000059604644\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9692000091075897\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9500999927520752\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9062000036239624\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10319999925792217\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8383999997377396\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9627000051736831\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9267000037431717\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10319999977946281\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9767000126838684\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9553999948501587\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9235000026226043\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.1010999996215105\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8030999958515167\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9616000056266785\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.931900002360344\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999979138374\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9782000148296356\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9607999980449676\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.93\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.08920000020414591\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8411999928951264\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9628000020980835\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9365000003576278\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10279999986290932\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9792000126838684\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9625999975204468\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9354999995231629\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10319999968633056\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8437999975681305\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9675000041723252\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9346999990940094\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10099999994039535\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9824000132083893\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9665000033378601\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9345000016689301\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 1 (MyModel_1)\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "n_epochs_list = [10, 20, 40, 80, 160, 320]\n",
    "batch_size_list = [100, 200]\n",
    "learning_rate_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "display_step = 1\n",
    "\n",
    "# Launch the graph\n",
    "model_1_dict = dict()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Training\n",
    "    n_epochs_dict = dict()\n",
    "    for n_epochs in n_epochs_list:\n",
    "        batch_size_dict = dict()\n",
    "        \n",
    "        for batch_size in batch_size_list:\n",
    "            learning_rate_dict = dict()\n",
    "            \n",
    "            for learning_rate in learning_rate_list:\n",
    "                accuracy_dict = dict()\n",
    "                \n",
    "                model = MyModel_1(sess=sess, learning_rate=learning_rate, batch_size=batch_size)\n",
    "                model.build_model()\n",
    "                \n",
    "                # Start\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                print(\"[ n_epochs : {}, batch_size : {}, learning_rate : {} ]\".format(n_epochs, batch_size, learning_rate))\n",
    "                \n",
    "                for i in range(n_epochs):\n",
    "                    total_loss = 0.\n",
    "                    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "                    # Loop over all batches\n",
    "                    for j in range(n_batches):\n",
    "                        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "                        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                        l, _ = model.train(x_batch=X_batch, y_batch=Y_batch)\n",
    "                        # Compute average loss\n",
    "                        total_loss += l\n",
    "                    # Display logs per epoch step\n",
    "                    # print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "                # print(\"Optimization Finished!\")\n",
    "\n",
    "                n_batches = int(mnist.test.num_examples/batch_size)\n",
    "                total_correct_preds = 0\n",
    "\n",
    "                for i in range(n_batches):\n",
    "                    X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "                    accuracy_batch = model.get_accuracy(x_batch=X_batch, y_batch=Y_batch)\n",
    "                    total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "                accuracy = total_correct_preds / n_batches\n",
    "                print('> Accuracy : {0}'.format(accuracy))\n",
    "                \n",
    "                learning_rate_dict[learning_rate] = accuracy\n",
    "            \n",
    "            batch_size_dict[batch_size] = learning_rate_dict\n",
    "            \n",
    "        n_epochs_dict[n_epochs] = batch_size_dict\n",
    "    \n",
    "    model_1_dict[\"model_1\"] = n_epochs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4t7PxbBKzmHy",
    "outputId": "9bde61e9-6edd-472e-e88e-ed1d5be13f3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_1': {10: {100: {0.0001: 0.8909000033140182,\n",
       "    0.001: 0.9503999990224838,\n",
       "    0.01: 0.8378999978303909,\n",
       "    0.1: 0.09579999979585409},\n",
       "   200: {0.0001: 0.8801999974250794,\n",
       "    0.001: 0.944399995803833,\n",
       "    0.01: 0.9552999913692475,\n",
       "    0.1: 0.11350000098347664}},\n",
       "  20: {100: {0.0001: 0.9155000030994416,\n",
       "    0.001: 0.9554000043869019,\n",
       "    0.01: 0.8800000023841857,\n",
       "    0.1: 0.11349999975413085},\n",
       "   200: {0.0001: 0.9062000036239624,\n",
       "    0.001: 0.9500999927520752,\n",
       "    0.01: 0.9692000091075897,\n",
       "    0.1: 0.19750000059604644}},\n",
       "  40: {100: {0.0001: 0.9267000037431717,\n",
       "    0.001: 0.9627000051736831,\n",
       "    0.01: 0.8383999997377396,\n",
       "    0.1: 0.10319999925792217},\n",
       "   200: {0.0001: 0.9235000026226043,\n",
       "    0.001: 0.9553999948501587,\n",
       "    0.01: 0.9767000126838684,\n",
       "    0.1: 0.10319999977946281}},\n",
       "  80: {100: {0.0001: 0.931900002360344,\n",
       "    0.001: 0.9616000056266785,\n",
       "    0.01: 0.8030999958515167,\n",
       "    0.1: 0.1010999996215105},\n",
       "   200: {0.0001: 0.93,\n",
       "    0.001: 0.9607999980449676,\n",
       "    0.01: 0.9782000148296356,\n",
       "    0.1: 0.11349999979138374}},\n",
       "  160: {100: {0.0001: 0.9365000003576278,\n",
       "    0.001: 0.9628000020980835,\n",
       "    0.01: 0.8411999928951264,\n",
       "    0.1: 0.08920000020414591},\n",
       "   200: {0.0001: 0.9354999995231629,\n",
       "    0.001: 0.9625999975204468,\n",
       "    0.01: 0.9792000126838684,\n",
       "    0.1: 0.10279999986290932}},\n",
       "  320: {100: {0.0001: 0.9346999990940094,\n",
       "    0.001: 0.9675000041723252,\n",
       "    0.01: 0.8437999975681305,\n",
       "    0.1: 0.10319999968633056},\n",
       "   200: {0.0001: 0.9345000016689301,\n",
       "    0.001: 0.9665000033378601,\n",
       "    0.01: 0.9824000132083893,\n",
       "    0.1: 0.10099999994039535}}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = json.dumps(model_1_dict)\n",
    "f = open(\"model_1_dict.json\", \"w\")\n",
    "f.write(json_data)\n",
    "f.close()\n",
    "\n",
    "model_1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykxWV2-XKYbX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999956786633\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9672000062465668\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.957900003194809\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9124000012874603\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.0981999995559454\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.970900011062622\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9518999934196473\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.8969999992847443\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.09820000000298024\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9742000091075897\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9629000037908554\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9260000056028366\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11350000023841858\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.97910001039505\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9556999933719635\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9190999972820282\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10279999993741512\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9780000096559525\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9674000060558319\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9334000009298324\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.1944999998807907\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9800000143051147\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9645999956130982\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9291000020503998\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10319999972358346\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.977400010228157\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9682000082731247\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9337000024318695\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11370000004768371\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9810000133514404\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9663000011444092\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9389999961853027\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10089999986812473\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9828000098466874\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9720000064373017\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9336000007390975\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999994039535\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9798000144958496\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9712000060081482\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9345000004768371\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.18120000049471854\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9822000104188919\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9730000102519989\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9400000041723251\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999949336052\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9812000143527985\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9705000042915344\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9376999986171722\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 2 (MyModel_2)\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "n_epochs_list = [10, 20, 40, 80, 160, 320]\n",
    "batch_size_list = [100, 200]\n",
    "learning_rate_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "# Launch the graph\n",
    "model_2_dict = dict()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Training\n",
    "    n_epochs_dict = dict()\n",
    "    for n_epochs in n_epochs_list:\n",
    "        batch_size_dict = dict()\n",
    "        \n",
    "        for batch_size in batch_size_list:\n",
    "            learning_rate_dict = dict()\n",
    "            \n",
    "            for learning_rate in learning_rate_list:\n",
    "                accuracy_dict = dict()\n",
    "                \n",
    "                model = MyModel_2(sess=sess, learning_rate=learning_rate, batch_size=batch_size)\n",
    "                model.build_model()\n",
    "                \n",
    "                # Start\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                print(\"[ n_epochs : {}, batch_size : {}, learning_rate : {} ]\".format(n_epochs, batch_size, learning_rate))\n",
    "                \n",
    "                for i in range(n_epochs):\n",
    "                    total_loss = 0.\n",
    "                    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "                    # Loop over all batches\n",
    "                    for j in range(n_batches):\n",
    "                        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "                        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                        l, _ = model.train(x_batch=X_batch, y_batch=Y_batch)\n",
    "                        # Compute average loss\n",
    "                        total_loss += l\n",
    "                    # Display logs per epoch step\n",
    "                    # print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "                # print(\"Optimization Finished!\")\n",
    "\n",
    "                n_batches = int(mnist.test.num_examples/batch_size)\n",
    "                total_correct_preds = 0\n",
    "\n",
    "                for i in range(n_batches):\n",
    "                    X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "                    accuracy_batch = model.get_accuracy(x_batch=X_batch, y_batch=Y_batch)\n",
    "                    total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "                accuracy = total_correct_preds / n_batches\n",
    "                print('> Accuracy : {0}'.format(accuracy))\n",
    "                \n",
    "                learning_rate_dict[learning_rate] = accuracy\n",
    "            \n",
    "            batch_size_dict[batch_size] = learning_rate_dict\n",
    "            \n",
    "        n_epochs_dict[n_epochs] = batch_size_dict\n",
    "    \n",
    "    model_2_dict[\"model_2\"] = n_epochs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_2': {10: {100: {0.0001: 0.9124000012874603,\n",
       "    0.001: 0.957900003194809,\n",
       "    0.01: 0.9672000062465668,\n",
       "    0.1: 0.11349999956786633},\n",
       "   200: {0.0001: 0.8969999992847443,\n",
       "    0.001: 0.9518999934196473,\n",
       "    0.01: 0.970900011062622,\n",
       "    0.1: 0.0981999995559454}},\n",
       "  20: {100: {0.0001: 0.9260000056028366,\n",
       "    0.001: 0.9629000037908554,\n",
       "    0.01: 0.9742000091075897,\n",
       "    0.1: 0.09820000000298024},\n",
       "   200: {0.0001: 0.9190999972820282,\n",
       "    0.001: 0.9556999933719635,\n",
       "    0.01: 0.97910001039505,\n",
       "    0.1: 0.11350000023841858}},\n",
       "  40: {100: {0.0001: 0.9334000009298324,\n",
       "    0.001: 0.9674000060558319,\n",
       "    0.01: 0.9780000096559525,\n",
       "    0.1: 0.10279999993741512},\n",
       "   200: {0.0001: 0.9291000020503998,\n",
       "    0.001: 0.9645999956130982,\n",
       "    0.01: 0.9800000143051147,\n",
       "    0.1: 0.1944999998807907}},\n",
       "  80: {100: {0.0001: 0.9337000024318695,\n",
       "    0.001: 0.9682000082731247,\n",
       "    0.01: 0.977400010228157,\n",
       "    0.1: 0.10319999972358346},\n",
       "   200: {0.0001: 0.9389999961853027,\n",
       "    0.001: 0.9663000011444092,\n",
       "    0.01: 0.9810000133514404,\n",
       "    0.1: 0.11370000004768371}},\n",
       "  160: {100: {0.0001: 0.9336000007390975,\n",
       "    0.001: 0.9720000064373017,\n",
       "    0.01: 0.9828000098466874,\n",
       "    0.1: 0.10089999986812473},\n",
       "   200: {0.0001: 0.9345000004768371,\n",
       "    0.001: 0.9712000060081482,\n",
       "    0.01: 0.9798000144958496,\n",
       "    0.1: 0.11349999994039535}},\n",
       "  320: {100: {0.0001: 0.9400000041723251,\n",
       "    0.001: 0.9730000102519989,\n",
       "    0.01: 0.9822000104188919,\n",
       "    0.1: 0.18120000049471854},\n",
       "   200: {0.0001: 0.9376999986171722,\n",
       "    0.001: 0.9705000042915344,\n",
       "    0.01: 0.9812000143527985,\n",
       "    0.1: 0.11349999949336052}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = json.dumps(model_2_dict)\n",
    "f = open(\"model_2_dict.json\", \"w\")\n",
    "f.write(json_data)\n",
    "f.close()\n",
    "\n",
    "model_2_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.1135000004991889\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9681000036001205\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9594000017642975\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9148000001907348\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.1010000005364418\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9716000044345856\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9535999965667724\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9085000014305115\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.20000000044703484\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9774000084400177\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9638000065088272\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9283000028133392\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.20490000009536743\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9757000100612641\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9623999953269958\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9277000045776367\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.1947000003606081\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9784000092744827\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9703000110387802\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9345999991893769\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10279999919235706\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9783000123500823\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9637999951839447\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9309000015258789\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999971687794\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9798000103235245\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9725000065565109\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9343000000715256\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10279999993741512\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9811000156402588\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9711000049114227\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9385999989509582\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.19520000025629997\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9837000095844268\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9730000096559525\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9415999978780747\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.19719999939203262\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9815000128746033\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.973000009059906\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9393000018596649\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10099999960511923\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9831000089645385\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9735000109672547\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.941700000166893\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10100000023841858\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9833000099658966\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9722000086307525\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9374000000953674\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 3 (MyModel_3)\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "n_epochs_list = [10, 20, 40, 80, 160, 320]\n",
    "batch_size_list = [100, 200]\n",
    "learning_rate_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "# Launch the graph\n",
    "model_3_dict = dict()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Training\n",
    "    n_epochs_dict = dict()\n",
    "    for n_epochs in n_epochs_list:\n",
    "        batch_size_dict = dict()\n",
    "        \n",
    "        for batch_size in batch_size_list:\n",
    "            learning_rate_dict = dict()\n",
    "            \n",
    "            for learning_rate in learning_rate_list:\n",
    "                accuracy_dict = dict()\n",
    "                \n",
    "                model = MyModel_3(sess=sess, learning_rate=learning_rate, batch_size=batch_size)\n",
    "                model.build_model()\n",
    "                \n",
    "                # Start\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                print(\"[ n_epochs : {}, batch_size : {}, learning_rate : {} ]\".format(n_epochs, batch_size, learning_rate))\n",
    "                \n",
    "                for i in range(n_epochs):\n",
    "                    total_loss = 0.\n",
    "                    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "                    # Loop over all batches\n",
    "                    for j in range(n_batches):\n",
    "                        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "                        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                        l, _ = model.train(x_batch=X_batch, y_batch=Y_batch)\n",
    "                        # Compute average loss\n",
    "                        total_loss += l\n",
    "                    # Display logs per epoch step\n",
    "                    # print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "                # print(\"Optimization Finished!\")\n",
    "\n",
    "                n_batches = int(mnist.test.num_examples/batch_size)\n",
    "                total_correct_preds = 0\n",
    "\n",
    "                for i in range(n_batches):\n",
    "                    X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "                    accuracy_batch = model.get_accuracy(x_batch=X_batch, y_batch=Y_batch)\n",
    "                    total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "                accuracy = total_correct_preds / n_batches\n",
    "                print('> Accuracy : {0}'.format(accuracy))\n",
    "                \n",
    "                learning_rate_dict[learning_rate] = accuracy\n",
    "            \n",
    "            batch_size_dict[batch_size] = learning_rate_dict\n",
    "            \n",
    "        n_epochs_dict[n_epochs] = batch_size_dict\n",
    "    \n",
    "    model_3_dict[\"model_3\"] = n_epochs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_3': {10: {100: {0.0001: 0.9148000001907348,\n",
       "    0.001: 0.9594000017642975,\n",
       "    0.01: 0.9681000036001205,\n",
       "    0.1: 0.1135000004991889},\n",
       "   200: {0.0001: 0.9085000014305115,\n",
       "    0.001: 0.9535999965667724,\n",
       "    0.01: 0.9716000044345856,\n",
       "    0.1: 0.1010000005364418}},\n",
       "  20: {100: {0.0001: 0.9283000028133392,\n",
       "    0.001: 0.9638000065088272,\n",
       "    0.01: 0.9774000084400177,\n",
       "    0.1: 0.20000000044703484},\n",
       "   200: {0.0001: 0.9277000045776367,\n",
       "    0.001: 0.9623999953269958,\n",
       "    0.01: 0.9757000100612641,\n",
       "    0.1: 0.20490000009536743}},\n",
       "  40: {100: {0.0001: 0.9345999991893769,\n",
       "    0.001: 0.9703000110387802,\n",
       "    0.01: 0.9784000092744827,\n",
       "    0.1: 0.1947000003606081},\n",
       "   200: {0.0001: 0.9309000015258789,\n",
       "    0.001: 0.9637999951839447,\n",
       "    0.01: 0.9783000123500823,\n",
       "    0.1: 0.10279999919235706}},\n",
       "  80: {100: {0.0001: 0.9343000000715256,\n",
       "    0.001: 0.9725000065565109,\n",
       "    0.01: 0.9798000103235245,\n",
       "    0.1: 0.11349999971687794},\n",
       "   200: {0.0001: 0.9385999989509582,\n",
       "    0.001: 0.9711000049114227,\n",
       "    0.01: 0.9811000156402588,\n",
       "    0.1: 0.10279999993741512}},\n",
       "  160: {100: {0.0001: 0.9415999978780747,\n",
       "    0.001: 0.9730000096559525,\n",
       "    0.01: 0.9837000095844268,\n",
       "    0.1: 0.19520000025629997},\n",
       "   200: {0.0001: 0.9393000018596649,\n",
       "    0.001: 0.973000009059906,\n",
       "    0.01: 0.9815000128746033,\n",
       "    0.1: 0.19719999939203262}},\n",
       "  320: {100: {0.0001: 0.941700000166893,\n",
       "    0.001: 0.9735000109672547,\n",
       "    0.01: 0.9831000089645385,\n",
       "    0.1: 0.10099999960511923},\n",
       "   200: {0.0001: 0.9374000000953674,\n",
       "    0.001: 0.9722000086307525,\n",
       "    0.01: 0.9833000099658966,\n",
       "    0.1: 0.10100000023841858}}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = json.dumps(model_3_dict)\n",
    "f = open(\"model_3_dict.json\", \"w\")\n",
    "f.write(json_data)\n",
    "f.close()\n",
    "\n",
    "model_3_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After saving model's dictionaries ...\n",
    "\"\"\"\n",
    "\n",
    "model_1_dict = dict()\n",
    "model_2_dict = dict()\n",
    "model_3_dict = dict()\n",
    "\n",
    "with open(\"./model_1_dict.json\") as f:\n",
    "    model_1_dict = json.load(f)\n",
    "\n",
    "with open(\"./model_2_dict.json\") as f:\n",
    "    model_2_dict = json.load(f)\n",
    "\n",
    "with open(\"./model_3_dict.json\") as f:\n",
    "    model_3_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_model_dict = dict()\n",
    "\n",
    "entire_model_dict[\"model_1\"] = model_1_dict[\"model_1\"]\n",
    "entire_model_dict[\"model_2\"] = model_2_dict[\"model_2\"]\n",
    "entire_model_dict[\"model_3\"] = model_3_dict[\"model_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc_list = list()\n",
    "\n",
    "for m_key, m_val in entire_model_dict.items():\n",
    "    for e_key, n_val in m_val.items():\n",
    "        for b_key, b_val in n_val.items():\n",
    "            for l_key, accuracy in b_val.items():\n",
    "                best_acc_list.append({\"model\": m_key, \"n_epochs\": e_key, \"batch_size\": b_key, \"learning_rate\": l_key, \"accuracy\": accuracy})\n",
    "\n",
    "best_acc_list = sorted(best_acc_list, key=lambda element: element[\"accuracy\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model': 'model_3',\n",
       "  'n_epochs': '160',\n",
       "  'batch_size': '100',\n",
       "  'learning_rate': '0.01',\n",
       "  'accuracy': 0.9837000095844268},\n",
       " {'model': 'model_3',\n",
       "  'n_epochs': '320',\n",
       "  'batch_size': '200',\n",
       "  'learning_rate': '0.01',\n",
       "  'accuracy': 0.9833000099658966},\n",
       " {'model': 'model_3',\n",
       "  'n_epochs': '320',\n",
       "  'batch_size': '100',\n",
       "  'learning_rate': '0.01',\n",
       "  'accuracy': 0.9831000089645385},\n",
       " {'model': 'model_2',\n",
       "  'n_epochs': '160',\n",
       "  'batch_size': '100',\n",
       "  'learning_rate': '0.01',\n",
       "  'accuracy': 0.9828000098466874},\n",
       " {'model': 'model_1',\n",
       "  'n_epochs': '320',\n",
       "  'batch_size': '200',\n",
       "  'learning_rate': '0.01',\n",
       "  'accuracy': 0.9824000132083893},\n",
       " {'model': 'model_2',\n",
       "  'n_epochs': '320',\n",
       "  'batch_size': '100',\n",
       "  'learning_rate': '0.01',\n",
       "  'accuracy': 0.9822000104188919},\n",
       " {'model': 'model_3',\n",
       "  'n_epochs': '160',\n",
       "  'batch_size': '200',\n",
       "  'learning_rate': '0.01',\n",
       "  'accuracy': 0.9815000128746033},\n",
       " {'model': 'model_2',\n",
       "  'n_epochs': '320',\n",
       "  'batch_size': '200',\n",
       "  'learning_rate': '0.01',\n",
       "  'accuracy': 0.9812000143527985},\n",
       " {'model': 'model_3',\n",
       "  'n_epochs': '80',\n",
       "  'batch_size': '200',\n",
       "  'learning_rate': '0.01',\n",
       "  'accuracy': 0.9811000156402588},\n",
       " {'model': 'model_2',\n",
       "  'n_epochs': '80',\n",
       "  'batch_size': '200',\n",
       "  'learning_rate': '0.01',\n",
       "  'accuracy': 0.9810000133514404}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_anal_data = {\"model_1_num\": 0, \"model_2_num\": 0, \"model_3_num\": 0}\n",
    "epoch_anal_data = {\"n_epochs_10\": 0, \"n_epochs_20\": 0, \"n_epochs_40\": 0, \"n_epochs_80\": 0, \"n_epochs_160\": 0, \"n_epochs_320\": 0}\n",
    "batch_size_anal_data = {\"batch_100\": 0, \"batch_200\": 0}\n",
    "learning_rate_anal_data = {\"0.1\": 0, \"0.01\": 0, \"0.001\": 0, \"0.0001\": 0}\n",
    "\n",
    "best_11_acc_dict_list= best_acc_list[:30]\n",
    "\n",
    "for acc_dict in best_11_acc_dict_list:\n",
    "    if acc_dict[\"model\"] == \"model_1\": model_anal_data[\"model_1_num\"] += 1\n",
    "    elif acc_dict[\"model\"] == \"model_2\": model_anal_data[\"model_2_num\"] += 1\n",
    "    elif acc_dict[\"model\"] == \"model_3\": model_anal_data[\"model_3_num\"] += 1\n",
    "    \n",
    "    if acc_dict[\"n_epochs\"] == \"10\": epoch_anal_data[\"n_epochs_10\"] += 1\n",
    "    elif acc_dict[\"n_epochs\"] == \"20\": epoch_anal_data[\"n_epochs_20\"] += 1\n",
    "    elif acc_dict[\"n_epochs\"] == \"40\": epoch_anal_data[\"n_epochs_40\"] += 1\n",
    "    elif acc_dict[\"n_epochs\"] == \"80\": epoch_anal_data[\"n_epochs_80\"] += 1\n",
    "    elif acc_dict[\"n_epochs\"] == \"160\": epoch_anal_data[\"n_epochs_160\"] += 1\n",
    "    elif acc_dict[\"n_epochs\"] == \"320\": epoch_anal_data[\"n_epochs_320\"] += 1\n",
    "        \n",
    "    if acc_dict[\"batch_size\"] == \"100\": batch_size_anal_data[\"batch_100\"] += 1\n",
    "    elif acc_dict[\"batch_size\"] == \"200\": batch_size_anal_data[\"batch_200\"] += 1\n",
    "    \n",
    "    if acc_dict[\"learning_rate\"] == \"0.1\": learning_rate_anal_data[\"0.1\"] += 1\n",
    "    elif acc_dict[\"learning_rate\"] == \"0.01\": learning_rate_anal_data[\"0.01\"] += 1\n",
    "    elif acc_dict[\"learning_rate\"] == \"0.001\": learning_rate_anal_data[\"0.001\"] += 1\n",
    "    elif acc_dict[\"learning_rate\"] == \"0.0001\": learning_rate_anal_data[\"0.0001\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = model_anal_data.values()\n",
    "x = np.arange(len(y))\n",
    "x_labels = [\"Model 1\", \"Model 2\", \"Model 3\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Count value by 'model'\")\n",
    "plt.bar(x, y)\n",
    "plt.xticks(x, x_labels)\n",
    "plt.yticks(sorted(y))\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Cumulative Value\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1  \n",
    " It can be seen that the frequency of getting the high accuracy increases from 'Model_1' to 'Model_3'. In this case, the number of hidden layers is 2, all the same. Therefore, this graph supports the above claim that the higher the number of features, the higher the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = epoch_anal_data.values()\n",
    "x = np.arange(len(y))\n",
    "x_labels = [\"10\", \"20\", \"40\", \"80\", \"160\", \"320\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Count value by 'n_epochs'\")\n",
    "plt.bar(x, y)\n",
    "plt.xticks(x, x_labels)\n",
    "plt.yticks(sorted(y))\n",
    "plt.xlabel(\"n_epochs\")\n",
    "plt.ylabel(\"Cumulative Value\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2\n",
    " According to the graph above, the higher the number of epochs, the higher the accuracy. This is a natural result that was derived of training the same data several times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = batch_size_anal_data.values()\n",
    "x = np.arange(len(y))\n",
    "x_labels = [\"100\", \"200\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Count value by 'Batch Size'\")\n",
    "plt.bar(x, y)\n",
    "plt.xticks(x, x_labels)\n",
    "plt.yticks(sorted(y))\n",
    "plt.xlabel(\"Batch Size\")\n",
    "plt.ylabel(\"Cumulative Value\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3\n",
    " In the above graph, there are only two cases (Batch Size = 100 or 200). Therefore, it is hard to judge how it affects the training. At the derived graph, there seems that has no big correlation. In fact, since 'Batch Size' is a parameter for how much to train at once, it will affect the learning speed and the computer memory rather than accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = learning_rate_anal_data.values()\n",
    "x = np.arange(len(y))\n",
    "x_labels = [\"0.1\", \"0.01\", \"0.001\", \"0.0001\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Count value by 'Learning Rate'\")\n",
    "plt.bar(x, y)\n",
    "plt.xticks(x, x_labels)\n",
    "plt.yticks(sorted(y))\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Cumulative Value\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 4\n",
    " According to the derived graph above, 'Learning Rate' does not show any special correlation. Perhaps the difference in each interval is so large, so we can not guess the accuracy at the subdivided interval. Obviously, the more learning rate converges to 0.01, the higher accuracy we will get.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of features is proportional to the accuracy.\n",
    "- The number of epochs is proportional to the accuracy.\n",
    "- In MNIST data set, The more learning rate converges to 0.01, the higher accuracy it has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model & Best Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best 1\n",
    "__(accuracy: 0.9837000095844268)__\n",
    "- 'number of layers': 2\n",
    "- 'hidden_layer_1_size' : 1024\n",
    "- 'hidden_layer_2_size' : 784\n",
    "- 'n_epochs': '160'\n",
    "- 'batch_size': '100'\n",
    "- 'learning_rate': '0.01'\n",
    "  \n",
    "#### Best 2\n",
    "__(accuracy: 0.9833000099658966)__\n",
    "- 'number of layers': 2\n",
    "- 'hidden_layer_1_size' : 1024\n",
    "- 'hidden_layer_2_size' : 784\n",
    "- 'n_epochs': '320'\n",
    "- 'batch_size': '200'\n",
    "- 'learning_rate': '0.01'\n",
    "  \n",
    "#### Best 3\n",
    "__(accuracy: 0.9831000089645385)__\n",
    "- 'number of layers': 2\n",
    "- 'hidden_layer_1_size' : 1024\n",
    "- 'hidden_layer_2_size' : 784\n",
    "- 'n_epochs': '320'\n",
    "- 'batch_size': '100'\n",
    "- 'learning_rate': '0.01'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "dnn_mnist.ipynb의 사본",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
