{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "id": "eMrOM1B838Eg",
    "outputId": "174aeabe-58c6-45e3-b857-426aae002ba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-374d72521340>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "DNN to classify MNIST handwritten digits\n",
    "'''\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "colab_type": "code",
    "id": "osqEHrEK38Em",
    "outputId": "2ced5e3b-27a0-4c3e-b637-affae02deca1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-a704a38ab237>:51: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Average loss epoch 0: 135.8106663305109\n",
      "Average loss epoch 1: 33.07767553502863\n",
      "Average loss epoch 2: 20.323474855422972\n",
      "Average loss epoch 3: 13.902497258294712\n",
      "Average loss epoch 4: 9.965045595610475\n",
      "Average loss epoch 5: 7.358891784230919\n",
      "Average loss epoch 6: 5.474251942507508\n",
      "Average loss epoch 7: 4.163528723437352\n",
      "Average loss epoch 8: 3.182201073814472\n",
      "Average loss epoch 9: 2.5385226775052376\n",
      "Average loss epoch 10: 1.9849248842868366\n",
      "Average loss epoch 11: 1.5254589312393576\n",
      "Average loss epoch 12: 1.225964889697639\n",
      "Average loss epoch 13: 0.9822812628534695\n",
      "Average loss epoch 14: 0.7658523709361795\n",
      "Average loss epoch 15: 0.6246198208679925\n",
      "Average loss epoch 16: 0.5215695654856197\n",
      "Average loss epoch 17: 0.46490002690898835\n",
      "Average loss epoch 18: 0.3999289285663105\n",
      "Average loss epoch 19: 0.32953024114579377\n",
      "Optimization Finished!\n",
      "Accuracy 0.9463\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task #1 & Task #2\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training \n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        n_batches = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            # Compute average loss\n",
    "            total_loss += l\n",
    "        # Display logs per epoch step\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        total_correct_preds += accuracy_batch   \n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBcFSYhv9wiA"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task #3\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "task3_data_dict = {\"Features\": np.array([256, 128, 64]),\n",
    "                   \"Accuracy\": np.zeros(3),\n",
    "                   \"Num of Hidden Layers\": np.zeros(3),\n",
    "                   \"Num of Learning Params\": np.zeros(3),\n",
    "                   \"Avg Loss (Last Epoch)\": np.zeros(3),\n",
    "                   \"Elapsed Time\": np.zeros(3)}\n",
    "\n",
    "task3_df = pd.DataFrame.from_dict(task3_data_dict)\n",
    "task3_df.index = task3_df[\"Features\"]\n",
    "del task3_df[\"Features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "lnUrGE6c4dQr",
    "outputId": "160d0d97-641c-410b-dfa5-00e5ee1bcf66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 160.86162128101697\n",
      "Average loss epoch 1: 34.096188970912586\n",
      "Average loss epoch 2: 20.73371030742472\n",
      "Average loss epoch 3: 14.302431708140807\n",
      "Average loss epoch 4: 10.400523157350042\n",
      "Average loss epoch 5: 7.721865692366295\n",
      "Average loss epoch 6: 5.7856823261618855\n",
      "Average loss epoch 7: 4.424016938563365\n",
      "Average loss epoch 8: 3.4341916809548785\n",
      "Average loss epoch 9: 2.6681965720259564\n",
      "Average loss epoch 10: 2.1047920663636686\n",
      "Average loss epoch 11: 1.6463948617910695\n",
      "Average loss epoch 12: 1.3296441090488085\n",
      "Average loss epoch 13: 1.0270987755473107\n",
      "Average loss epoch 14: 0.8384445363209733\n",
      "Average loss epoch 15: 0.6921892414488527\n",
      "Average loss epoch 16: 0.5857040391102663\n",
      "Average loss epoch 17: 0.48609939847797395\n",
      "Average loss epoch 18: 0.4299562169102687\n",
      "Average loss epoch 19: 0.3741052600311888\n",
      "Optimization Finished!\n",
      "Accuracy 0.9475\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task #3 - 1 : DNN with two hidden layers, each layer has 256 units (as in the given code)\n",
    "\"\"\"\n",
    "\n",
    "# Pandas Params\n",
    "row_index = 256\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "task3_df.loc[row_index, \"Num of Hidden Layers\"] = 2\n",
    "task3_df.loc[row_index, \"Num of Learning Params\"] = n_input * n_hidden_1 * n_hidden_2\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    start = time.time()\n",
    "    # Training \n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        n_batches = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            # Compute average loss\n",
    "            total_loss += l\n",
    "        # Display logs per epoch step\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        \n",
    "        if (j == (n_batches - 1)):\n",
    "            task3_df.loc[row_index, \"Avg Loss (Last Epoch)\"] = total_loss / n_batches\n",
    "            \n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    task3_df.loc[row_index, \"Elapsed Time\"] = end - start\n",
    "\n",
    "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        total_correct_preds += accuracy_batch   \n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "    \n",
    "    task3_df.loc[row_index, \"Accuracy\"] = total_correct_preds / mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "Y_XbqwgoKUKA",
    "outputId": "3723b71f-0ef9-4db2-fe5e-b534a3d26e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 560.8089004100452\n",
      "Average loss epoch 1: 118.7150104453347\n",
      "Average loss epoch 2: 71.90258411927657\n",
      "Average loss epoch 3: 50.22454301183874\n",
      "Average loss epoch 4: 37.56080601442944\n",
      "Average loss epoch 5: 29.241417313272304\n",
      "Average loss epoch 6: 23.035642646764384\n",
      "Average loss epoch 7: 18.52054607440125\n",
      "Average loss epoch 8: 15.094883438132026\n",
      "Average loss epoch 9: 12.36318474125266\n",
      "Average loss epoch 10: 10.12348026864032\n",
      "Average loss epoch 11: 8.343385827657224\n",
      "Average loss epoch 12: 7.263522472793784\n",
      "Average loss epoch 13: 6.035478309174858\n",
      "Average loss epoch 14: 5.122128925956597\n",
      "Average loss epoch 15: 4.454072027382025\n",
      "Average loss epoch 16: 3.8035533240468564\n",
      "Average loss epoch 17: 3.2434293466820363\n",
      "Average loss epoch 18: 2.7647905182688706\n",
      "Average loss epoch 19: 2.3500110400437055\n",
      "Optimization Finished!\n",
      "Accuracy 0.9254\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task #3 - 2 : DNN with three hidden layers, each layer has 128 units\n",
    "\"\"\"\n",
    "\n",
    "# Pandas Params\n",
    "row_index = 128\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of features\n",
    "n_hidden_2 = 128 # 2nd layer number of features\n",
    "n_hidden_3 = 128 # 3rd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "task3_df.loc[row_index, \"Num of Hidden Layers\"] = 3\n",
    "task3_df.loc[row_index, \"Num of Learning Params\"] = n_input * n_hidden_1 * n_hidden_2 * n_hidden_3\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Hidden layer\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    start = time.time()\n",
    "    # Training \n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        n_batches = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            # Compute average loss\n",
    "            total_loss += l\n",
    "        # Display logs per epoch step\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        \n",
    "        if (j == (n_batches - 1)):\n",
    "            task3_df.loc[row_index, \"Avg Loss (Last Epoch)\"] = total_loss / n_batches\n",
    "            \n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    task3_df.loc[row_index, \"Elapsed Time\"] = end - start\n",
    "\n",
    "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        total_correct_preds += accuracy_batch   \n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "    \n",
    "    task3_df.loc[row_index, \"Accuracy\"] = total_correct_preds / mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "BcFdVlTJL8Re",
    "outputId": "be45691f-9a21-423e-9d7a-0a78cda6eb92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss epoch 0: 1116.4598465798117\n",
      "Average loss epoch 1: 193.05631949684837\n",
      "Average loss epoch 2: 104.29994050459429\n",
      "Average loss epoch 3: 68.4742736937783\n",
      "Average loss epoch 4: 48.72823608745228\n",
      "Average loss epoch 5: 37.268853923624214\n",
      "Average loss epoch 6: 29.310892461863432\n",
      "Average loss epoch 7: 23.731789291121743\n",
      "Average loss epoch 8: 19.613103564435786\n",
      "Average loss epoch 9: 16.473175085891377\n",
      "Average loss epoch 10: 13.982375352165915\n",
      "Average loss epoch 11: 11.977295897223733\n",
      "Average loss epoch 12: 10.445814942717552\n",
      "Average loss epoch 13: 9.136606003479525\n",
      "Average loss epoch 14: 8.09996989025311\n",
      "Average loss epoch 15: 7.326248303001577\n",
      "Average loss epoch 16: 6.546369616389274\n",
      "Average loss epoch 17: 5.920628056766635\n",
      "Average loss epoch 18: 5.359115442895167\n",
      "Average loss epoch 19: 4.9954118120052255\n",
      "Optimization Finished!\n",
      "Accuracy 0.8923\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task #3 - 3 : DNN with four hidden layers, each layer has 64 units\n",
    "\"\"\"\n",
    "\n",
    "# Pandas Params\n",
    "row_index = 64\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 64 # 1st layer number of features\n",
    "n_hidden_2 = 64 # 2nd layer number of features\n",
    "n_hidden_3 = 64 # 3rd layer number of features\n",
    "n_hidden_4 = 64 # 4th layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "task3_df.loc[row_index, \"Num of Hidden Layers\"] = 4\n",
    "task3_df.loc[row_index, \"Num of Learning Params\"] = n_input * n_hidden_1 * n_hidden_2 * n_hidden_3 * n_hidden_4\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [batch_size, n_input])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Hidden layer\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3)\n",
    "    # Hidden layer\n",
    "    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])\n",
    "    layer_4 = tf.nn.relu(layer_4)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_4, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(X, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=Y))\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    start = time.time()\n",
    "    # Training \n",
    "    for i in range(n_epochs):\n",
    "        total_loss = 0.\n",
    "        n_batches = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for j in range(n_batches):\n",
    "            X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            # Compute average loss\n",
    "            total_loss += l\n",
    "        # Display logs per epoch step\n",
    "        print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "        \n",
    "        if (j == (n_batches - 1)):\n",
    "            task3_df.loc[row_index, \"Avg Loss (Last Epoch)\"] = total_loss / n_batches\n",
    "            \n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    task3_df.loc[row_index, \"Elapsed Time\"] = end - start\n",
    "\n",
    "    correct_preds = tf.equal(tf.argmax(pred, axis=1), tf.argmax(Y, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "    \n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={X: X_batch, Y:Y_batch}) \n",
    "        total_correct_preds += accuracy_batch   \n",
    "    \n",
    "    print('Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples))\n",
    "    \n",
    "    task3_df.loc[row_index, \"Accuracy\"] = total_correct_preds / mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "S6lUG0RUM2ee",
    "outputId": "7609097d-5a7c-4059-e105-9cbaaaa45fed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Avg Loss (Last Epoch)</th>\n",
       "      <th>Elapsed Time</th>\n",
       "      <th>Num of Hidden Layers</th>\n",
       "      <th>Num of Learning Params</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.9475</td>\n",
       "      <td>0.374105</td>\n",
       "      <td>12.412959</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.138022e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.9254</td>\n",
       "      <td>2.350011</td>\n",
       "      <td>13.456423</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.644167e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.8923</td>\n",
       "      <td>4.995412</td>\n",
       "      <td>14.470979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.315334e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy  Avg Loss (Last Epoch)  Elapsed Time  Num of Hidden Layers  \\\n",
       "Features                                                                        \n",
       "256         0.9475               0.374105     12.412959                   2.0   \n",
       "128         0.9254               2.350011     13.456423                   3.0   \n",
       "64          0.8923               4.995412     14.470979                   4.0   \n",
       "\n",
       "          Num of Learning Params  \n",
       "Features                          \n",
       "256                 5.138022e+07  \n",
       "128                 1.644167e+09  \n",
       "64                  1.315334e+10  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task3_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KTW98RN9PKiH",
    "outputId": "aba5c792-7bd3-42f3-bd20-0a60468c0a87"
   },
   "source": [
    "Feature 의 갯수가 적고, Layer 가 깊어질 수록 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task #4\n",
    "\"\"\"\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TF4XI_AmiUWv"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models\n",
    "\"\"\"\n",
    "all_model_dict = dict()\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# 784 -> 512 -> 256 -> 10\n",
    "class MyModel_1(object):\n",
    "    \n",
    "    def __init__(self, sess, learning_rate, batch_size):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_1_size = 512\n",
    "        self.hidden_2_size = 256\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[self.batch_size, n_input])\n",
    "        self.Y = tf.placeholder(tf.float32, shape=[self.batch_size, n_classes])\n",
    "        \n",
    "        self.pred = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        self.correct_preds = None\n",
    "        self.accuracy = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(\"Layer1\"):\n",
    "            W1 = tf.Variable(tf.random_normal([n_input, self.hidden_1_size]))\n",
    "            b1 = tf.Variable(tf.random_normal([self.hidden_1_size]))\n",
    "            layer1 = tf.add(tf.matmul(self.X, W1), b1)\n",
    "            layer1 = tf.nn.relu(layer1)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer2\"):\n",
    "            W2 = tf.Variable(tf.random_normal([self.hidden_1_size, self.hidden_2_size]))\n",
    "            b2 = tf.Variable(tf.random_normal([self.hidden_2_size]))\n",
    "            layer2 = tf.add(tf.matmul(layer1, W2), b2)\n",
    "            layer2 = tf.nn.relu(layer2)\n",
    "        \n",
    "        with tf.variable_scope(\"Layer3\"):\n",
    "            W3 = tf.Variable(tf.random_normal([self.hidden_2_size, n_classes]))\n",
    "            b3 = tf.Variable(tf.random_normal([n_classes]))\n",
    "            layer3 = tf.add(tf.matmul(layer2, W3), b3)\n",
    "        \n",
    "        self.pred = layer3\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.Y))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.correct_preds = tf.equal(tf.argmax(self.pred, axis=1), tf.argmax(self.Y, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_preds, tf.float32))\n",
    "    \n",
    "    def train(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.loss, self.optimizer], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
    "    \n",
    "    def get_accuracy(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.accuracy], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
    "\n",
    "\n",
    "# 784 -> 784 -> 512 -> 10\n",
    "class MyModel_2(object):\n",
    "    \n",
    "    def __init__(self, sess, learning_rate, batch_size):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_1_size = 784\n",
    "        self.hidden_2_size = 512 \n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[self.batch_size, n_input])\n",
    "        self.Y = tf.placeholder(tf.float32, shape=[self.batch_size, n_classes])\n",
    "        \n",
    "        self.pred = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        self.correct_preds = None\n",
    "        self.accuracy = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(\"Layer1\"):\n",
    "            W1 = tf.Variable(tf.random_normal([n_input, self.hidden_1_size]))\n",
    "            b1 = tf.Variable(tf.random_normal([self.hidden_1_size]))\n",
    "            layer1 = tf.add(tf.matmul(self.X, W1), b1)\n",
    "            layer1 = tf.nn.relu(layer1)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer2\"):\n",
    "            W2 = tf.Variable(tf.random_normal([self.hidden_1_size, self.hidden_2_size]))\n",
    "            b2 = tf.Variable(tf.random_normal([self.hidden_2_size]))\n",
    "            layer2 = tf.add(tf.matmul(layer1, W2), b2)\n",
    "            layer2 = tf.nn.relu(layer2)\n",
    "        \n",
    "        with tf.variable_scope(\"Layer3\"):\n",
    "            W3 = tf.Variable(tf.random_normal([self.hidden_2_size, n_classes]))\n",
    "            b3 = tf.Variable(tf.random_normal([n_classes]))\n",
    "            layer3 = tf.add(tf.matmul(layer2, W3), b3)\n",
    "        \n",
    "        self.pred = layer3\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.Y))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.correct_preds = tf.equal(tf.argmax(self.pred, axis=1), tf.argmax(self.Y, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_preds, tf.float32))\n",
    "        \n",
    "    def train(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.loss, self.optimizer], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
    "    \n",
    "    def get_accuracy(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.accuracy], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
    "    \n",
    "# 784 -> 1024 -> 784 -> 10\n",
    "class MyModel_3(object):\n",
    "    \n",
    "    def __init__(self, sess, learning_rate, batch_size):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_1_size = 1024\n",
    "        self.hidden_2_size = 784\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[self.batch_size, n_input])\n",
    "        self.Y = tf.placeholder(tf.float32, shape=[self.batch_size, n_classes])\n",
    "        \n",
    "        self.pred = None\n",
    "        self.loss = None\n",
    "        self.optimizer = None\n",
    "        \n",
    "        self.correct_preds = None\n",
    "        self.accuracy = None\n",
    "        \n",
    "    def build_model(self):\n",
    "        with tf.variable_scope(\"Layer1\"):\n",
    "            W1 = tf.Variable(tf.random_normal([n_input, self.hidden_1_size]))\n",
    "            b1 = tf.Variable(tf.random_normal([self.hidden_1_size]))\n",
    "            layer1 = tf.add(tf.matmul(self.X, W1), b1)\n",
    "            layer1 = tf.nn.relu(layer1)\n",
    "            \n",
    "        with tf.variable_scope(\"Layer2\"):\n",
    "            W2 = tf.Variable(tf.random_normal([self.hidden_1_size, self.hidden_2_size]))\n",
    "            b2 = tf.Variable(tf.random_normal([self.hidden_2_size]))\n",
    "            layer2 = tf.add(tf.matmul(layer1, W2), b2)\n",
    "            layer2 = tf.nn.relu(layer2)\n",
    "        \n",
    "        with tf.variable_scope(\"Layer3\"):\n",
    "            W3 = tf.Variable(tf.random_normal([self.hidden_2_size, n_classes]))\n",
    "            b3 = tf.Variable(tf.random_normal([n_classes]))\n",
    "            layer3 = tf.add(tf.matmul(layer2, W3), b3)\n",
    "        \n",
    "        self.pred = layer3\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.pred, labels=self.Y))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.correct_preds = tf.equal(tf.argmax(self.pred, axis=1), tf.argmax(self.Y, axis=1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_preds, tf.float32))\n",
    "        \n",
    "    def train(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.loss, self.optimizer], feed_dict={self.X: x_batch, self.Y: y_batch})\n",
    "    \n",
    "    def get_accuracy(self, x_batch, y_batch):\n",
    "        return self.sess.run(fetches=[self.accuracy], feed_dict={self.X: x_batch, self.Y: y_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1119
    },
    "colab_type": "code",
    "id": "CPylf-EVPobI",
    "outputId": "224fea92-2b6b-4291-e624-454bdc889d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.09579999979585409\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8378999978303909\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9503999990224838\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.8909000033140182\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11350000098347664\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9552999913692475\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.944399995803833\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.8801999974250794\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999975413085\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8800000023841857\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9554000043869019\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9155000030994416\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.19750000059604644\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9692000091075897\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9500999927520752\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9062000036239624\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10319999925792217\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8383999997377396\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9627000051736831\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9267000037431717\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10319999977946281\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9767000126838684\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9553999948501587\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9235000026226043\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.1010999996215105\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8030999958515167\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9616000056266785\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.931900002360344\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999979138374\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9782000148296356\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9607999980449676\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.93\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.08920000020414591\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8411999928951264\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9628000020980835\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9365000003576278\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10279999986290932\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9792000126838684\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9625999975204468\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9354999995231629\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10319999968633056\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.8437999975681305\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9675000041723252\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9346999990940094\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10099999994039535\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9824000132083893\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9665000033378601\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9345000016689301\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 1 (MyModel_1)\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "n_epochs_list = [10, 20, 40, 80, 160, 320]\n",
    "batch_size_list = [100, 200]\n",
    "learning_rate_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "display_step = 1\n",
    "\n",
    "# Launch the graph\n",
    "model_1_dict = dict()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Training\n",
    "    n_epochs_dict = dict()\n",
    "    for n_epochs in n_epochs_list:\n",
    "        batch_size_dict = dict()\n",
    "        \n",
    "        for batch_size in batch_size_list:\n",
    "            learning_rate_dict = dict()\n",
    "            \n",
    "            for learning_rate in learning_rate_list:\n",
    "                accuracy_dict = dict()\n",
    "                \n",
    "                model = MyModel_1(sess=sess, learning_rate=learning_rate, batch_size=batch_size)\n",
    "                model.build_model()\n",
    "                \n",
    "                # Start\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                print(\"[ n_epochs : {}, batch_size : {}, learning_rate : {} ]\".format(n_epochs, batch_size, learning_rate))\n",
    "                \n",
    "                for i in range(n_epochs):\n",
    "                    total_loss = 0.\n",
    "                    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "                    # Loop over all batches\n",
    "                    for j in range(n_batches):\n",
    "                        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "                        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                        l, _ = model.train(x_batch=X_batch, y_batch=Y_batch)\n",
    "                        # Compute average loss\n",
    "                        total_loss += l\n",
    "                    # Display logs per epoch step\n",
    "                    # print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "                # print(\"Optimization Finished!\")\n",
    "\n",
    "                n_batches = int(mnist.test.num_examples/batch_size)\n",
    "                total_correct_preds = 0\n",
    "\n",
    "                for i in range(n_batches):\n",
    "                    X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "                    accuracy_batch = model.get_accuracy(x_batch=X_batch, y_batch=Y_batch)\n",
    "                    total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "                accuracy = total_correct_preds / n_batches\n",
    "                print('> Accuracy : {0}'.format(accuracy))\n",
    "                \n",
    "                learning_rate_dict[learning_rate] = accuracy\n",
    "            \n",
    "            batch_size_dict[batch_size] = learning_rate_dict\n",
    "            \n",
    "        n_epochs_dict[n_epochs] = batch_size_dict\n",
    "    \n",
    "    model_1_dict[\"model_1\"] = n_epochs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4t7PxbBKzmHy",
    "outputId": "9bde61e9-6edd-472e-e88e-ed1d5be13f3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_1': {10: {100: {0.0001: 0.8909000033140182,\n",
       "    0.001: 0.9503999990224838,\n",
       "    0.01: 0.8378999978303909,\n",
       "    0.1: 0.09579999979585409},\n",
       "   200: {0.0001: 0.8801999974250794,\n",
       "    0.001: 0.944399995803833,\n",
       "    0.01: 0.9552999913692475,\n",
       "    0.1: 0.11350000098347664}},\n",
       "  20: {100: {0.0001: 0.9155000030994416,\n",
       "    0.001: 0.9554000043869019,\n",
       "    0.01: 0.8800000023841857,\n",
       "    0.1: 0.11349999975413085},\n",
       "   200: {0.0001: 0.9062000036239624,\n",
       "    0.001: 0.9500999927520752,\n",
       "    0.01: 0.9692000091075897,\n",
       "    0.1: 0.19750000059604644}},\n",
       "  40: {100: {0.0001: 0.9267000037431717,\n",
       "    0.001: 0.9627000051736831,\n",
       "    0.01: 0.8383999997377396,\n",
       "    0.1: 0.10319999925792217},\n",
       "   200: {0.0001: 0.9235000026226043,\n",
       "    0.001: 0.9553999948501587,\n",
       "    0.01: 0.9767000126838684,\n",
       "    0.1: 0.10319999977946281}},\n",
       "  80: {100: {0.0001: 0.931900002360344,\n",
       "    0.001: 0.9616000056266785,\n",
       "    0.01: 0.8030999958515167,\n",
       "    0.1: 0.1010999996215105},\n",
       "   200: {0.0001: 0.93,\n",
       "    0.001: 0.9607999980449676,\n",
       "    0.01: 0.9782000148296356,\n",
       "    0.1: 0.11349999979138374}},\n",
       "  160: {100: {0.0001: 0.9365000003576278,\n",
       "    0.001: 0.9628000020980835,\n",
       "    0.01: 0.8411999928951264,\n",
       "    0.1: 0.08920000020414591},\n",
       "   200: {0.0001: 0.9354999995231629,\n",
       "    0.001: 0.9625999975204468,\n",
       "    0.01: 0.9792000126838684,\n",
       "    0.1: 0.10279999986290932}},\n",
       "  320: {100: {0.0001: 0.9346999990940094,\n",
       "    0.001: 0.9675000041723252,\n",
       "    0.01: 0.8437999975681305,\n",
       "    0.1: 0.10319999968633056},\n",
       "   200: {0.0001: 0.9345000016689301,\n",
       "    0.001: 0.9665000033378601,\n",
       "    0.01: 0.9824000132083893,\n",
       "    0.1: 0.10099999994039535}}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = json.dumps(model_1_dict)\n",
    "f = open(\"model_1_dict.json\", \"w\")\n",
    "f.write(json_data)\n",
    "f.close()\n",
    "\n",
    "model_1_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ykxWV2-XKYbX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999956786633\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9672000062465668\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.957900003194809\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9124000012874603\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.0981999995559454\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.970900011062622\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9518999934196473\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.8969999992847443\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.09820000000298024\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9742000091075897\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9629000037908554\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9260000056028366\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11350000023841858\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.97910001039505\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9556999933719635\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9190999972820282\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10279999993741512\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9780000096559525\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9674000060558319\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9334000009298324\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.1944999998807907\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9800000143051147\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9645999956130982\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9291000020503998\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10319999972358346\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.977400010228157\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9682000082731247\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9337000024318695\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11370000004768371\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9810000133514404\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9663000011444092\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9389999961853027\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10089999986812473\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9828000098466874\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9720000064373017\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9336000007390975\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999994039535\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9798000144958496\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9712000060081482\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9345000004768371\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.18120000049471854\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9822000104188919\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9730000102519989\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9400000041723251\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999949336052\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9812000143527985\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9705000042915344\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9376999986171722\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 2 (MyModel_2)\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "n_epochs_list = [10, 20, 40, 80, 160, 320]\n",
    "batch_size_list = [100, 200]\n",
    "learning_rate_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "# Launch the graph\n",
    "model_2_dict = dict()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Training\n",
    "    n_epochs_dict = dict()\n",
    "    for n_epochs in n_epochs_list:\n",
    "        batch_size_dict = dict()\n",
    "        \n",
    "        for batch_size in batch_size_list:\n",
    "            learning_rate_dict = dict()\n",
    "            \n",
    "            for learning_rate in learning_rate_list:\n",
    "                accuracy_dict = dict()\n",
    "                \n",
    "                model = MyModel_2(sess=sess, learning_rate=learning_rate, batch_size=batch_size)\n",
    "                model.build_model()\n",
    "                \n",
    "                # Start\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                print(\"[ n_epochs : {}, batch_size : {}, learning_rate : {} ]\".format(n_epochs, batch_size, learning_rate))\n",
    "                \n",
    "                for i in range(n_epochs):\n",
    "                    total_loss = 0.\n",
    "                    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "                    # Loop over all batches\n",
    "                    for j in range(n_batches):\n",
    "                        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "                        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                        l, _ = model.train(x_batch=X_batch, y_batch=Y_batch)\n",
    "                        # Compute average loss\n",
    "                        total_loss += l\n",
    "                    # Display logs per epoch step\n",
    "                    # print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "                # print(\"Optimization Finished!\")\n",
    "\n",
    "                n_batches = int(mnist.test.num_examples/batch_size)\n",
    "                total_correct_preds = 0\n",
    "\n",
    "                for i in range(n_batches):\n",
    "                    X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "                    accuracy_batch = model.get_accuracy(x_batch=X_batch, y_batch=Y_batch)\n",
    "                    total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "                accuracy = total_correct_preds / n_batches\n",
    "                print('> Accuracy : {0}'.format(accuracy))\n",
    "                \n",
    "                learning_rate_dict[learning_rate] = accuracy\n",
    "            \n",
    "            batch_size_dict[batch_size] = learning_rate_dict\n",
    "            \n",
    "        n_epochs_dict[n_epochs] = batch_size_dict\n",
    "    \n",
    "    model_2_dict[\"model_2\"] = n_epochs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_2': {10: {100: {0.0001: 0.9124000012874603,\n",
       "    0.001: 0.957900003194809,\n",
       "    0.01: 0.9672000062465668,\n",
       "    0.1: 0.11349999956786633},\n",
       "   200: {0.0001: 0.8969999992847443,\n",
       "    0.001: 0.9518999934196473,\n",
       "    0.01: 0.970900011062622,\n",
       "    0.1: 0.0981999995559454}},\n",
       "  20: {100: {0.0001: 0.9260000056028366,\n",
       "    0.001: 0.9629000037908554,\n",
       "    0.01: 0.9742000091075897,\n",
       "    0.1: 0.09820000000298024},\n",
       "   200: {0.0001: 0.9190999972820282,\n",
       "    0.001: 0.9556999933719635,\n",
       "    0.01: 0.97910001039505,\n",
       "    0.1: 0.11350000023841858}},\n",
       "  40: {100: {0.0001: 0.9334000009298324,\n",
       "    0.001: 0.9674000060558319,\n",
       "    0.01: 0.9780000096559525,\n",
       "    0.1: 0.10279999993741512},\n",
       "   200: {0.0001: 0.9291000020503998,\n",
       "    0.001: 0.9645999956130982,\n",
       "    0.01: 0.9800000143051147,\n",
       "    0.1: 0.1944999998807907}},\n",
       "  80: {100: {0.0001: 0.9337000024318695,\n",
       "    0.001: 0.9682000082731247,\n",
       "    0.01: 0.977400010228157,\n",
       "    0.1: 0.10319999972358346},\n",
       "   200: {0.0001: 0.9389999961853027,\n",
       "    0.001: 0.9663000011444092,\n",
       "    0.01: 0.9810000133514404,\n",
       "    0.1: 0.11370000004768371}},\n",
       "  160: {100: {0.0001: 0.9336000007390975,\n",
       "    0.001: 0.9720000064373017,\n",
       "    0.01: 0.9828000098466874,\n",
       "    0.1: 0.10089999986812473},\n",
       "   200: {0.0001: 0.9345000004768371,\n",
       "    0.001: 0.9712000060081482,\n",
       "    0.01: 0.9798000144958496,\n",
       "    0.1: 0.11349999994039535}},\n",
       "  320: {100: {0.0001: 0.9400000041723251,\n",
       "    0.001: 0.9730000102519989,\n",
       "    0.01: 0.9822000104188919,\n",
       "    0.1: 0.18120000049471854},\n",
       "   200: {0.0001: 0.9376999986171722,\n",
       "    0.001: 0.9705000042915344,\n",
       "    0.01: 0.9812000143527985,\n",
       "    0.1: 0.11349999949336052}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = json.dumps(model_2_dict)\n",
    "f = open(\"model_2_dict.json\", \"w\")\n",
    "f.write(json_data)\n",
    "f.close()\n",
    "\n",
    "model_2_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.1135000004991889\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9681000036001205\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9594000017642975\n",
      "[ n_epochs : 10, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9148000001907348\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.1010000005364418\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9716000044345856\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9535999965667724\n",
      "[ n_epochs : 10, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9085000014305115\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.20000000044703484\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9774000084400177\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9638000065088272\n",
      "[ n_epochs : 20, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9283000028133392\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.20490000009536743\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9757000100612641\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9623999953269958\n",
      "[ n_epochs : 20, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9277000045776367\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.1947000003606081\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9784000092744827\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9703000110387802\n",
      "[ n_epochs : 40, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9345999991893769\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10279999919235706\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9783000123500823\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9637999951839447\n",
      "[ n_epochs : 40, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9309000015258789\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.11349999971687794\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9798000103235245\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9725000065565109\n",
      "[ n_epochs : 80, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9343000000715256\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10279999993741512\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9811000156402588\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9711000049114227\n",
      "[ n_epochs : 80, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9385999989509582\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.19520000025629997\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9837000095844268\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9730000096559525\n",
      "[ n_epochs : 160, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9415999978780747\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.19719999939203262\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9815000128746033\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.973000009059906\n",
      "[ n_epochs : 160, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9393000018596649\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10099999960511923\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9831000089645385\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9735000109672547\n",
      "[ n_epochs : 320, batch_size : 100, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.941700000166893\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.1 ]\n",
      "> Accuracy : 0.10100000023841858\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.01 ]\n",
      "> Accuracy : 0.9833000099658966\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.001 ]\n",
      "> Accuracy : 0.9722000086307525\n",
      "[ n_epochs : 320, batch_size : 200, learning_rate : 0.0001 ]\n",
      "> Accuracy : 0.9374000000953674\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model 3 (MyModel_3)\n",
    "\"\"\"\n",
    "\n",
    "# Parameters\n",
    "n_epochs_list = [10, 20, 40, 80, 160, 320]\n",
    "batch_size_list = [100, 200]\n",
    "learning_rate_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "# Launch the graph\n",
    "model_3_dict = dict()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Training\n",
    "    n_epochs_dict = dict()\n",
    "    for n_epochs in n_epochs_list:\n",
    "        batch_size_dict = dict()\n",
    "        \n",
    "        for batch_size in batch_size_list:\n",
    "            learning_rate_dict = dict()\n",
    "            \n",
    "            for learning_rate in learning_rate_list:\n",
    "                accuracy_dict = dict()\n",
    "                \n",
    "                model = MyModel_3(sess=sess, learning_rate=learning_rate, batch_size=batch_size)\n",
    "                model.build_model()\n",
    "                \n",
    "                # Start\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                print(\"[ n_epochs : {}, batch_size : {}, learning_rate : {} ]\".format(n_epochs, batch_size, learning_rate))\n",
    "                \n",
    "                for i in range(n_epochs):\n",
    "                    total_loss = 0.\n",
    "                    n_batches = int(mnist.train.num_examples/batch_size)\n",
    "                    # Loop over all batches\n",
    "                    for j in range(n_batches):\n",
    "                        X_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "                        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                        l, _ = model.train(x_batch=X_batch, y_batch=Y_batch)\n",
    "                        # Compute average loss\n",
    "                        total_loss += l\n",
    "                    # Display logs per epoch step\n",
    "                    # print('Average loss epoch {0}: {1}'.format(i, total_loss/n_batches))\n",
    "\n",
    "                # print(\"Optimization Finished!\")\n",
    "\n",
    "                n_batches = int(mnist.test.num_examples/batch_size)\n",
    "                total_correct_preds = 0\n",
    "\n",
    "                for i in range(n_batches):\n",
    "                    X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "                    accuracy_batch = model.get_accuracy(x_batch=X_batch, y_batch=Y_batch)\n",
    "                    total_correct_preds += accuracy_batch[0]\n",
    "\n",
    "                accuracy = total_correct_preds / n_batches\n",
    "                print('> Accuracy : {0}'.format(accuracy))\n",
    "                \n",
    "                learning_rate_dict[learning_rate] = accuracy\n",
    "            \n",
    "            batch_size_dict[batch_size] = learning_rate_dict\n",
    "            \n",
    "        n_epochs_dict[n_epochs] = batch_size_dict\n",
    "    \n",
    "    model_3_dict[\"model_3\"] = n_epochs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_3': {10: {100: {0.0001: 0.9148000001907348,\n",
       "    0.001: 0.9594000017642975,\n",
       "    0.01: 0.9681000036001205,\n",
       "    0.1: 0.1135000004991889},\n",
       "   200: {0.0001: 0.9085000014305115,\n",
       "    0.001: 0.9535999965667724,\n",
       "    0.01: 0.9716000044345856,\n",
       "    0.1: 0.1010000005364418}},\n",
       "  20: {100: {0.0001: 0.9283000028133392,\n",
       "    0.001: 0.9638000065088272,\n",
       "    0.01: 0.9774000084400177,\n",
       "    0.1: 0.20000000044703484},\n",
       "   200: {0.0001: 0.9277000045776367,\n",
       "    0.001: 0.9623999953269958,\n",
       "    0.01: 0.9757000100612641,\n",
       "    0.1: 0.20490000009536743}},\n",
       "  40: {100: {0.0001: 0.9345999991893769,\n",
       "    0.001: 0.9703000110387802,\n",
       "    0.01: 0.9784000092744827,\n",
       "    0.1: 0.1947000003606081},\n",
       "   200: {0.0001: 0.9309000015258789,\n",
       "    0.001: 0.9637999951839447,\n",
       "    0.01: 0.9783000123500823,\n",
       "    0.1: 0.10279999919235706}},\n",
       "  80: {100: {0.0001: 0.9343000000715256,\n",
       "    0.001: 0.9725000065565109,\n",
       "    0.01: 0.9798000103235245,\n",
       "    0.1: 0.11349999971687794},\n",
       "   200: {0.0001: 0.9385999989509582,\n",
       "    0.001: 0.9711000049114227,\n",
       "    0.01: 0.9811000156402588,\n",
       "    0.1: 0.10279999993741512}},\n",
       "  160: {100: {0.0001: 0.9415999978780747,\n",
       "    0.001: 0.9730000096559525,\n",
       "    0.01: 0.9837000095844268,\n",
       "    0.1: 0.19520000025629997},\n",
       "   200: {0.0001: 0.9393000018596649,\n",
       "    0.001: 0.973000009059906,\n",
       "    0.01: 0.9815000128746033,\n",
       "    0.1: 0.19719999939203262}},\n",
       "  320: {100: {0.0001: 0.941700000166893,\n",
       "    0.001: 0.9735000109672547,\n",
       "    0.01: 0.9831000089645385,\n",
       "    0.1: 0.10099999960511923},\n",
       "   200: {0.0001: 0.9374000000953674,\n",
       "    0.001: 0.9722000086307525,\n",
       "    0.01: 0.9833000099658966,\n",
       "    0.1: 0.10100000023841858}}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = json.dumps(model_3_dict)\n",
    "f = open(\"model_3_dict.json\", \"w\")\n",
    "f.write(json_data)\n",
    "f.close()\n",
    "\n",
    "model_3_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_model_dict = dict()\n",
    "\n",
    "entire_model_dict[\"model_1\"] = model_1_dict[\"model_1\"]\n",
    "entire_model_dict[\"model_2\"] = model_2_dict[\"model_2\"]\n",
    "entire_model_dict[\"model_3\"] = model_3_dict[\"model_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc_list = list()\n",
    "\n",
    "for m_key, m_val in entire_model_dict.items():\n",
    "    for e_key, n_val in m_val.items():\n",
    "        for b_key, b_val in n_val.items():\n",
    "            for l_key, accuracy in b_val.items():\n",
    "                best_acc_list.append({\"model\": m_key, \"n_epochs\": e_key, \"batch_size\": b_key, \"learning_rate\": l_key, \"accuracy\": accuracy})\n",
    "\n",
    "best_acc_list = sorted(best_acc_list, key=lambda element: element[\"accuracy\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.9837000095844268,\n",
       "  'batch_size': 100,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_3',\n",
       "  'n_epochs': 160},\n",
       " {'accuracy': 0.9833000099658966,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_3',\n",
       "  'n_epochs': 320},\n",
       " {'accuracy': 0.9831000089645385,\n",
       "  'batch_size': 100,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_3',\n",
       "  'n_epochs': 320},\n",
       " {'accuracy': 0.9828000098466874,\n",
       "  'batch_size': 100,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_2',\n",
       "  'n_epochs': 160},\n",
       " {'accuracy': 0.9824000132083893,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_1',\n",
       "  'n_epochs': 320},\n",
       " {'accuracy': 0.9822000104188919,\n",
       "  'batch_size': 100,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_2',\n",
       "  'n_epochs': 320},\n",
       " {'accuracy': 0.9815000128746033,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_3',\n",
       "  'n_epochs': 160},\n",
       " {'accuracy': 0.9812000143527985,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_2',\n",
       "  'n_epochs': 320},\n",
       " {'accuracy': 0.9811000156402588,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_3',\n",
       "  'n_epochs': 80},\n",
       " {'accuracy': 0.9810000133514404,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_2',\n",
       "  'n_epochs': 80},\n",
       " {'accuracy': 0.9800000143051147,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_2',\n",
       "  'n_epochs': 40},\n",
       " {'accuracy': 0.9798000144958496,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_2',\n",
       "  'n_epochs': 160},\n",
       " {'accuracy': 0.9798000103235245,\n",
       "  'batch_size': 100,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_3',\n",
       "  'n_epochs': 80},\n",
       " {'accuracy': 0.9792000126838684,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_1',\n",
       "  'n_epochs': 160},\n",
       " {'accuracy': 0.97910001039505,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_2',\n",
       "  'n_epochs': 20},\n",
       " {'accuracy': 0.9784000092744827,\n",
       "  'batch_size': 100,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_3',\n",
       "  'n_epochs': 40},\n",
       " {'accuracy': 0.9783000123500823,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_3',\n",
       "  'n_epochs': 40},\n",
       " {'accuracy': 0.9782000148296356,\n",
       "  'batch_size': 200,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_1',\n",
       "  'n_epochs': 80},\n",
       " {'accuracy': 0.9780000096559525,\n",
       "  'batch_size': 100,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_2',\n",
       "  'n_epochs': 40},\n",
       " {'accuracy': 0.977400010228157,\n",
       "  'batch_size': 100,\n",
       "  'learning_rate': 0.01,\n",
       "  'model': 'model_2',\n",
       "  'n_epochs': 80}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "dnn_mnist.ipynb의 사본",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
