{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Project_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 4.008907\n",
      "Epoch: 0002 cost = 3.926097\n",
      "Epoch: 0003 cost = 3.844792\n",
      "Epoch: 0004 cost = 3.763762\n",
      "Epoch: 0005 cost = 3.681812\n",
      "Epoch: 0006 cost = 3.597792\n",
      "Epoch: 0007 cost = 3.510710\n",
      "Epoch: 0008 cost = 3.419755\n",
      "Epoch: 0009 cost = 3.324250\n",
      "Epoch: 0010 cost = 3.223634\n",
      "Epoch: 0011 cost = 3.117485\n",
      "Epoch: 0012 cost = 3.005560\n",
      "Epoch: 0013 cost = 2.887846\n",
      "Epoch: 0014 cost = 2.764598\n",
      "Epoch: 0015 cost = 2.636387\n",
      "Epoch: 0016 cost = 2.504138\n",
      "Epoch: 0017 cost = 2.369148\n",
      "Epoch: 0018 cost = 2.233070\n",
      "Epoch: 0019 cost = 2.097843\n",
      "Epoch: 0020 cost = 1.965582\n",
      "Epoch: 0021 cost = 1.838394\n",
      "Epoch: 0022 cost = 1.718150\n",
      "Epoch: 0023 cost = 1.606258\n",
      "Epoch: 0024 cost = 1.503475\n",
      "Epoch: 0025 cost = 1.409801\n",
      "Epoch: 0026 cost = 1.324508\n",
      "Epoch: 0027 cost = 1.246323\n",
      "Epoch: 0028 cost = 1.173705\n",
      "Epoch: 0029 cost = 1.105172\n",
      "Epoch: 0030 cost = 1.039602\n",
      "Epoch: 0031 cost = 0.976456\n",
      "Epoch: 0032 cost = 0.915834\n",
      "Epoch: 0033 cost = 0.858366\n",
      "Epoch: 0034 cost = 0.804951\n",
      "Epoch: 0035 cost = 0.756411\n",
      "Epoch: 0036 cost = 0.713156\n",
      "Epoch: 0037 cost = 0.674990\n",
      "Epoch: 0038 cost = 0.641182\n",
      "Epoch: 0039 cost = 0.610725\n",
      "Epoch: 0040 cost = 0.582582\n",
      "Epoch: 0041 cost = 0.555821\n",
      "Epoch: 0042 cost = 0.529705\n",
      "Epoch: 0043 cost = 0.503749\n",
      "Epoch: 0044 cost = 0.477765\n",
      "Epoch: 0045 cost = 0.451873\n",
      "Epoch: 0046 cost = 0.426437\n",
      "Epoch: 0047 cost = 0.401921\n",
      "Epoch: 0048 cost = 0.378749\n",
      "Epoch: 0049 cost = 0.357206\n",
      "Epoch: 0050 cost = 0.337405\n",
      "Epoch: 0051 cost = 0.319311\n",
      "Epoch: 0052 cost = 0.302770\n",
      "Epoch: 0053 cost = 0.287560\n",
      "Epoch: 0054 cost = 0.273439\n",
      "Epoch: 0055 cost = 0.260196\n",
      "Epoch: 0056 cost = 0.247680\n",
      "Epoch: 0057 cost = 0.235806\n",
      "Epoch: 0058 cost = 0.224539\n",
      "Epoch: 0059 cost = 0.213873\n",
      "Epoch: 0060 cost = 0.203802\n",
      "Epoch: 0061 cost = 0.194303\n",
      "Epoch: 0062 cost = 0.185321\n",
      "Epoch: 0063 cost = 0.176778\n",
      "Epoch: 0064 cost = 0.168598\n",
      "Epoch: 0065 cost = 0.160732\n",
      "Epoch: 0066 cost = 0.153186\n",
      "Epoch: 0067 cost = 0.146003\n",
      "Epoch: 0068 cost = 0.139249\n",
      "Epoch: 0069 cost = 0.132970\n",
      "Epoch: 0070 cost = 0.127179\n",
      "Epoch: 0071 cost = 0.121850\n",
      "Epoch: 0072 cost = 0.116933\n",
      "Epoch: 0073 cost = 0.112365\n",
      "Epoch: 0074 cost = 0.108083\n",
      "Epoch: 0075 cost = 0.104035\n",
      "Epoch: 0076 cost = 0.100184\n",
      "Epoch: 0077 cost = 0.096505\n",
      "Epoch: 0078 cost = 0.092984\n",
      "Epoch: 0079 cost = 0.089614\n",
      "Epoch: 0080 cost = 0.086393\n",
      "Epoch: 0081 cost = 0.083316\n",
      "Epoch: 0082 cost = 0.080379\n",
      "Epoch: 0083 cost = 0.077577\n",
      "Epoch: 0084 cost = 0.074906\n",
      "Epoch: 0085 cost = 0.072362\n",
      "Epoch: 0086 cost = 0.069941\n",
      "Epoch: 0087 cost = 0.067642\n",
      "Epoch: 0088 cost = 0.065459\n",
      "Epoch: 0089 cost = 0.063389\n",
      "Epoch: 0090 cost = 0.061426\n",
      "Epoch: 0091 cost = 0.059562\n",
      "Epoch: 0092 cost = 0.057790\n",
      "Epoch: 0093 cost = 0.056103\n",
      "Epoch: 0094 cost = 0.054494\n",
      "Epoch: 0095 cost = 0.052956\n",
      "Epoch: 0096 cost = 0.051485\n",
      "Epoch: 0097 cost = 0.050075\n",
      "Epoch: 0098 cost = 0.048723\n",
      "Epoch: 0099 cost = 0.047426\n",
      "Epoch: 0100 cost = 0.046182\n",
      "Optimization finished\n",
      "\n",
      "=== Predictions ===\n",
      "Input: ['woo ', 'dee ', 'col ', 'loa ', 'lov ', 'dea ', 'dov ', 'cel ', 'lif ', 'kee ']\n",
      "Predicted: ['wood', 'deep', 'cold', 'load', 'love', 'deap', 'dove', 'celd', 'life', 'keep']\n",
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Autocompletion of the last character of words\n",
    "  Given the first three letters of a four-letters word, learn to predict the last letter \n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "         'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "         'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "         'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# index array of characters in vocab\n",
    "v_map = {n: i for i, n in enumerate(vocab)}\n",
    "v_len = len(v_map)\n",
    "\n",
    "# training data (character sequences)\n",
    "# wor -> X, d -> Y\n",
    "# woo -> X, d -> Y\n",
    "training_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
    "test_data = ['wood', 'deep', 'cold', 'load', 'love', 'dear', 'dove', 'cell', 'life', 'keep']\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        # Indices of the first three alphabets of the words\n",
    "        # [22, 14, 17] [22, 14, 14] [3, 4, 4] [3, 8, 21] ...\n",
    "        input = [v_map[n] for n in seq[:-1]]\n",
    "        # Indices of the last alphabet of the words\n",
    "        # 3, 3, 15, 4, 3 ...\n",
    "        target = v_map[seq[-1]]\n",
    "\n",
    "        # One-hot encoding of the inputs into the sequences of 26-dimensional vectors\n",
    "        # [0, 1, 2] ==>\n",
    "        # [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "        #  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "        #  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
    "        input_batch.append(np.eye(v_len)[input])\n",
    "        \n",
    "        # We don't apply one-hot encoding for the output,  \n",
    "        # since we'll use sparse_softmax_cross_entropy_with_logits\n",
    "        # as our loss function\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_hidden = 10\n",
    "total_epoch = 100\n",
    "n_step = 3 # the length of the input sequence\n",
    "n_input = n_class = v_len # the size of each input\n",
    "\n",
    "\"\"\"\n",
    "  Phase 1: Create the computation graph\n",
    "\"\"\"\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "#                output (pred. of forth letter)\n",
    "#                 | (W,b)\n",
    "#                outputs (hidden)   \n",
    "#       |    |    | \n",
    "# RNN: [t1]-[t2]-[t3]\n",
    "#       x1   x2   x3\n",
    "\n",
    "# Create an LSTM cell\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "# Apply dropout for regularization\n",
    "#cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.75)\n",
    "\n",
    "# Create the RNN\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "# outputs : [batch_size, max_time, cell.output_size]\n",
    "\n",
    "# Transform the output of RNN to create output values\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "# [batch_size, cell.output_size]\n",
    "model = tf.matmul(outputs, W) + b\n",
    "# [batch_size, n_classes]\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\"\"\"\n",
    "  Phase 2: Train the model\n",
    "\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    input_batch, target_batch = make_batch(training_data)\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        _, loss = sess.run([optimizer, cost],\n",
    "                           feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    print('Optimization finished')\n",
    "\n",
    "    \"\"\"\n",
    "      Make predictions\n",
    "    \"\"\"\n",
    "    seq_data = test_data\n",
    "    prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, Y), tf.float32))\n",
    "\n",
    "    input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "    predict, accuracy_val = sess.run([prediction, accuracy],\n",
    "                                     feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "    predicted = []\n",
    "    for idx, val in enumerate(seq_data):\n",
    "        last_char = vocab[predict[idx]]\n",
    "        predicted.append(val[:3] + last_char)\n",
    "\n",
    "    print('\\n=== Predictions ===')\n",
    "    print('Input:', [w[:3] + ' ' for w in seq_data])\n",
    "    print('Predicted:', predicted)\n",
    "    print('Accuracy:', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 3.538748\n",
      "Epoch: 0002 cost = 3.478555\n",
      "Epoch: 0003 cost = 3.421258\n",
      "Epoch: 0004 cost = 3.365892\n",
      "Epoch: 0005 cost = 3.311715\n",
      "Epoch: 0006 cost = 3.258047\n",
      "Epoch: 0007 cost = 3.204259\n",
      "Epoch: 0008 cost = 3.149777\n",
      "Epoch: 0009 cost = 3.094093\n",
      "Epoch: 0010 cost = 3.036782\n",
      "Epoch: 0011 cost = 2.977501\n",
      "Epoch: 0012 cost = 2.915974\n",
      "Epoch: 0013 cost = 2.851979\n",
      "Epoch: 0014 cost = 2.785343\n",
      "Epoch: 0015 cost = 2.715939\n",
      "Epoch: 0016 cost = 2.643705\n",
      "Epoch: 0017 cost = 2.568656\n",
      "Epoch: 0018 cost = 2.490892\n",
      "Epoch: 0019 cost = 2.410587\n",
      "Epoch: 0020 cost = 2.327984\n",
      "Epoch: 0021 cost = 2.243371\n",
      "Epoch: 0022 cost = 2.157051\n",
      "Epoch: 0023 cost = 2.069323\n",
      "Epoch: 0024 cost = 1.980497\n",
      "Epoch: 0025 cost = 1.890940\n",
      "Epoch: 0026 cost = 1.801136\n",
      "Epoch: 0027 cost = 1.711720\n",
      "Epoch: 0028 cost = 1.623516\n",
      "Epoch: 0029 cost = 1.537597\n",
      "Epoch: 0030 cost = 1.455236\n",
      "Epoch: 0031 cost = 1.377656\n",
      "Epoch: 0032 cost = 1.305584\n",
      "Epoch: 0033 cost = 1.238847\n",
      "Epoch: 0034 cost = 1.176355\n",
      "Epoch: 0035 cost = 1.116584\n",
      "Epoch: 0036 cost = 1.058234\n",
      "Epoch: 0037 cost = 1.000714\n",
      "Epoch: 0038 cost = 0.944273\n",
      "Epoch: 0039 cost = 0.889785\n",
      "Epoch: 0040 cost = 0.838264\n",
      "Epoch: 0041 cost = 0.790377\n",
      "Epoch: 0042 cost = 0.746244\n",
      "Epoch: 0043 cost = 0.705458\n",
      "Epoch: 0044 cost = 0.667280\n",
      "Epoch: 0045 cost = 0.630958\n",
      "Epoch: 0046 cost = 0.596028\n",
      "Epoch: 0047 cost = 0.562482\n",
      "Epoch: 0048 cost = 0.530745\n",
      "Epoch: 0049 cost = 0.501422\n",
      "Epoch: 0050 cost = 0.474961\n",
      "Epoch: 0051 cost = 0.451435\n",
      "Epoch: 0052 cost = 0.430585\n",
      "Epoch: 0053 cost = 0.412017\n",
      "Epoch: 0054 cost = 0.395348\n",
      "Epoch: 0055 cost = 0.380251\n",
      "Epoch: 0056 cost = 0.366443\n",
      "Epoch: 0057 cost = 0.353665\n",
      "Epoch: 0058 cost = 0.341693\n",
      "Epoch: 0059 cost = 0.330356\n",
      "Epoch: 0060 cost = 0.319559\n",
      "Epoch: 0061 cost = 0.309296\n",
      "Epoch: 0062 cost = 0.299628\n",
      "Epoch: 0063 cost = 0.290645\n",
      "Epoch: 0064 cost = 0.282407\n",
      "Epoch: 0065 cost = 0.274895\n",
      "Epoch: 0066 cost = 0.268016\n",
      "Epoch: 0067 cost = 0.261644\n",
      "Epoch: 0068 cost = 0.255686\n",
      "Epoch: 0069 cost = 0.250103\n",
      "Epoch: 0070 cost = 0.244896\n",
      "Epoch: 0071 cost = 0.240063\n",
      "Epoch: 0072 cost = 0.235572\n",
      "Epoch: 0073 cost = 0.231364\n",
      "Epoch: 0074 cost = 0.227384\n",
      "Epoch: 0075 cost = 0.223606\n",
      "Epoch: 0076 cost = 0.220041\n",
      "Epoch: 0077 cost = 0.216707\n",
      "Epoch: 0078 cost = 0.213614\n",
      "Epoch: 0079 cost = 0.210740\n",
      "Epoch: 0080 cost = 0.208052\n",
      "Epoch: 0081 cost = 0.205522\n",
      "Epoch: 0082 cost = 0.203144\n",
      "Epoch: 0083 cost = 0.200918\n",
      "Epoch: 0084 cost = 0.198844\n",
      "Epoch: 0085 cost = 0.196905\n",
      "Epoch: 0086 cost = 0.195077\n",
      "Epoch: 0087 cost = 0.193343\n",
      "Epoch: 0088 cost = 0.191697\n",
      "Epoch: 0089 cost = 0.190140\n",
      "Epoch: 0090 cost = 0.188673\n",
      "Epoch: 0091 cost = 0.187286\n",
      "Epoch: 0092 cost = 0.185966\n",
      "Epoch: 0093 cost = 0.184704\n",
      "Epoch: 0094 cost = 0.183500\n",
      "Epoch: 0095 cost = 0.182357\n",
      "Epoch: 0096 cost = 0.181273\n",
      "Epoch: 0097 cost = 0.180243\n",
      "Epoch: 0098 cost = 0.179257\n",
      "Epoch: 0099 cost = 0.178312\n",
      "Epoch: 0100 cost = 0.177408\n",
      "Optimization finished\n",
      "\n",
      "=== Predictions ===\n",
      "Input: ['wo_d', 'de_p', 'co_d', 'lo_d', 'lo_e', 'de_r', 'do_e', 'ce_l', 'li_e', 'ke_p']\n",
      "Predicted: ['word', 'deep', 'cold', 'load', 'love', 'deer', 'dove', 'ceol', 'live', 'keep']\n",
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Autocompletion of the last character of words\n",
    "  Given the first three letters of a four-letters word, learn to predict the last letter\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "         'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "         'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "         'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# index array of characters in vocab\n",
    "v_map = {n: i for i, n in enumerate(vocab)}\n",
    "v_len = len(v_map)\n",
    "\n",
    "# training data (character sequences)\n",
    "# wor -> X, d -> Y\n",
    "# woo -> X, d -> Y\n",
    "training_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
    "test_data = ['wood', 'deep', 'cold', 'load', 'love', 'dear', 'dove', 'cell', 'life', 'keep']\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        # Indices of the first three alphabets of the words\n",
    "        # [22, 14, 17] [22, 14, 14] [3, 4, 4] [3, 8, 21] ...\n",
    "        input = [v_map[seq[0]], v_map[seq[1]], v_map[seq[-1]]]\n",
    "        # Indices of the last alphabet of the words\n",
    "        # 3, 3, 15, 4, 3 ...\n",
    "        target = v_map[seq[-2]]\n",
    "\n",
    "        # One-hot encoding of the inputs into the sequences of 26-dimensional vectors\n",
    "        # [0, 1, 2] ==>\n",
    "        # [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "        #  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "        #  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
    "        input_batch.append(np.eye(v_len)[input])\n",
    "\n",
    "        # We don't apply one-hot encoding for the output,\n",
    "        # since we'll use sparse_softmax_cross_entropy_with_logits\n",
    "        # as our loss function\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_hidden = 10\n",
    "total_epoch = 100\n",
    "n_step = 3 # the length of the input sequence\n",
    "n_input = n_class = v_len # the size of each input\n",
    "\n",
    "\"\"\"\n",
    "  Phase 1: Create the computation graph\n",
    "\"\"\"\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "#                output (pred. of third letter)\n",
    "#                 | (W,b)\n",
    "#                outputs (hidden)\n",
    "#       |    |    |\n",
    "# RNN: [t1]-[t2]-[t3]\n",
    "#       x1   x2   x3\n",
    "\n",
    "# Create an LSTM cell\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "# Apply dropout for regularization\n",
    "#cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.75)\n",
    "\n",
    "# Create the RNN\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "# outputs : [batch_size, max_time, cell.output_size]\n",
    "\n",
    "# Transform the output of RNN to create output values\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "# [batch_size, cell.output_size]\n",
    "model = tf.matmul(outputs, W) + b\n",
    "# [batch_size, n_classes]\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\"\"\"\n",
    "  Phase 2: Train the model\n",
    "\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    input_batch, target_batch = make_batch(training_data)\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        _, loss = sess.run([optimizer, cost],\n",
    "                           feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    print('Optimization finished')\n",
    "\n",
    "    \"\"\"\n",
    "      Make predictions\n",
    "    \"\"\"\n",
    "    seq_data = test_data\n",
    "    prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, Y), tf.float32))\n",
    "\n",
    "    input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "    predict, accuracy_val = sess.run([prediction, accuracy],\n",
    "                                     feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "    predicted = []\n",
    "    for idx, val in enumerate(seq_data):\n",
    "        middle_char = vocab[predict[idx]]\n",
    "        predicted.append(val[:2] + middle_char + val[-1])\n",
    "\n",
    "    print('\\n=== Predictions ===')\n",
    "    print('Input:', [\"{}_{}\".format(w[:2], w[-1]) for w in seq_data])\n",
    "    print('Predicted:', predicted)\n",
    "    print('Accuracy:', accuracy_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
