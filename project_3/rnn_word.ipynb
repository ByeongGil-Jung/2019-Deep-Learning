{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Project_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 3.608380\n",
      "Epoch: 0002 cost = 3.513700\n",
      "Epoch: 0003 cost = 3.419682\n",
      "Epoch: 0004 cost = 3.325208\n",
      "Epoch: 0005 cost = 3.229174\n",
      "Epoch: 0006 cost = 3.130659\n",
      "Epoch: 0007 cost = 3.028875\n",
      "Epoch: 0008 cost = 2.923135\n",
      "Epoch: 0009 cost = 2.812870\n",
      "Epoch: 0010 cost = 2.697656\n",
      "Epoch: 0011 cost = 2.577245\n",
      "Epoch: 0012 cost = 2.451602\n",
      "Epoch: 0013 cost = 2.320984\n",
      "Epoch: 0014 cost = 2.186035\n",
      "Epoch: 0015 cost = 2.047878\n",
      "Epoch: 0016 cost = 1.908164\n",
      "Epoch: 0017 cost = 1.769062\n",
      "Epoch: 0018 cost = 1.633132\n",
      "Epoch: 0019 cost = 1.503088\n",
      "Epoch: 0020 cost = 1.381399\n",
      "Epoch: 0021 cost = 1.269831\n",
      "Epoch: 0022 cost = 1.169086\n",
      "Epoch: 0023 cost = 1.078635\n",
      "Epoch: 0024 cost = 0.996863\n",
      "Epoch: 0025 cost = 0.921522\n",
      "Epoch: 0026 cost = 0.850343\n",
      "Epoch: 0027 cost = 0.781618\n",
      "Epoch: 0028 cost = 0.714549\n",
      "Epoch: 0029 cost = 0.649317\n",
      "Epoch: 0030 cost = 0.586881\n",
      "Epoch: 0031 cost = 0.528626\n",
      "Epoch: 0032 cost = 0.475938\n",
      "Epoch: 0033 cost = 0.429831\n",
      "Epoch: 0034 cost = 0.390670\n",
      "Epoch: 0035 cost = 0.358069\n",
      "Epoch: 0036 cost = 0.330981\n",
      "Epoch: 0037 cost = 0.307948\n",
      "Epoch: 0038 cost = 0.287446\n",
      "Epoch: 0039 cost = 0.268193\n",
      "Epoch: 0040 cost = 0.249366\n",
      "Epoch: 0041 cost = 0.230670\n",
      "Epoch: 0042 cost = 0.212280\n",
      "Epoch: 0043 cost = 0.194683\n",
      "Epoch: 0044 cost = 0.178479\n",
      "Epoch: 0045 cost = 0.164177\n",
      "Epoch: 0046 cost = 0.152050\n",
      "Epoch: 0047 cost = 0.142068\n",
      "Epoch: 0048 cost = 0.133916\n",
      "Epoch: 0049 cost = 0.127083\n",
      "Epoch: 0050 cost = 0.121002\n",
      "Epoch: 0051 cost = 0.115191\n",
      "Epoch: 0052 cost = 0.109357\n",
      "Epoch: 0053 cost = 0.103431\n",
      "Epoch: 0054 cost = 0.097523\n",
      "Epoch: 0055 cost = 0.091834\n",
      "Epoch: 0056 cost = 0.086564\n",
      "Epoch: 0057 cost = 0.081844\n",
      "Epoch: 0058 cost = 0.077714\n",
      "Epoch: 0059 cost = 0.074124\n",
      "Epoch: 0060 cost = 0.070971\n",
      "Epoch: 0061 cost = 0.068125\n",
      "Epoch: 0062 cost = 0.065467\n",
      "Epoch: 0063 cost = 0.062908\n",
      "Epoch: 0064 cost = 0.060399\n",
      "Epoch: 0065 cost = 0.057928\n",
      "Epoch: 0066 cost = 0.055512\n",
      "Epoch: 0067 cost = 0.053184\n",
      "Epoch: 0068 cost = 0.050976\n",
      "Epoch: 0069 cost = 0.048914\n",
      "Epoch: 0070 cost = 0.047009\n",
      "Epoch: 0071 cost = 0.045258\n",
      "Epoch: 0072 cost = 0.043647\n",
      "Epoch: 0073 cost = 0.042155\n",
      "Epoch: 0074 cost = 0.040761\n",
      "Epoch: 0075 cost = 0.039444\n",
      "Epoch: 0076 cost = 0.038188\n",
      "Epoch: 0077 cost = 0.036985\n",
      "Epoch: 0078 cost = 0.035828\n",
      "Epoch: 0079 cost = 0.034717\n",
      "Epoch: 0080 cost = 0.033653\n",
      "Epoch: 0081 cost = 0.032636\n",
      "Epoch: 0082 cost = 0.031669\n",
      "Epoch: 0083 cost = 0.030752\n",
      "Epoch: 0084 cost = 0.029882\n",
      "Epoch: 0085 cost = 0.029058\n",
      "Epoch: 0086 cost = 0.028277\n",
      "Epoch: 0087 cost = 0.027535\n",
      "Epoch: 0088 cost = 0.026829\n",
      "Epoch: 0089 cost = 0.026156\n",
      "Epoch: 0090 cost = 0.025514\n",
      "Epoch: 0091 cost = 0.024899\n",
      "Epoch: 0092 cost = 0.024310\n",
      "Epoch: 0093 cost = 0.023747\n",
      "Epoch: 0094 cost = 0.023207\n",
      "Epoch: 0095 cost = 0.022688\n",
      "Epoch: 0096 cost = 0.022191\n",
      "Epoch: 0097 cost = 0.021715\n",
      "Epoch: 0098 cost = 0.021257\n",
      "Epoch: 0099 cost = 0.020818\n",
      "Epoch: 0100 cost = 0.020395\n",
      "Optimization finished\n",
      "\n",
      "=== Predictions ===\n",
      "Input: ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'coo ', 'loa ', 'lov ', 'kis ', 'kin ']\n",
      "Predicted: ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Autocompletion of the last character of words\n",
    "  Given the first three letters of a four-letters word, learn to predict the last letter \n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "         'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "         'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "         'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# index array of characters in vocab\n",
    "v_map = {n: i for i, n in enumerate(vocab)}\n",
    "v_len = len(v_map)\n",
    "\n",
    "# training data (character sequences)\n",
    "# wor -> X, d -> Y\n",
    "# woo -> X, d -> Y\n",
    "training_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
    "test_data = ['wood', 'deep', 'cold', 'load', 'love', 'dear', 'dove', 'cell', 'life', 'keep']\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        # Indices of the first three alphabets of the words\n",
    "        # [22, 14, 17] [22, 14, 14] [3, 4, 4] [3, 8, 21] ...\n",
    "        input = [v_map[n] for n in seq[:-1]]\n",
    "        # Indices of the last alphabet of the words\n",
    "        # 3, 3, 15, 4, 3 ...\n",
    "        target = v_map[seq[-1]]\n",
    "\n",
    "        # One-hot encoding of the inputs into the sequences of 26-dimensional vectors\n",
    "        # [0, 1, 2] ==>\n",
    "        # [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "        #  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "        #  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
    "        input_batch.append(np.eye(v_len)[input])\n",
    "        \n",
    "        # We don't apply one-hot encoding for the output,  \n",
    "        # since we'll use sparse_softmax_cross_entropy_with_logits\n",
    "        # as our loss function\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_hidden = 10\n",
    "total_epoch = 100\n",
    "n_step = 3 # the length of the input sequence\n",
    "n_input = n_class = v_len # the size of each input\n",
    "\n",
    "\"\"\"\n",
    "  Phase 1: Create the computation graph\n",
    "\"\"\"\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "#                output (pred. of forth letter)\n",
    "#                 | (W,b)\n",
    "#                outputs (hidden)   \n",
    "#       |    |    | \n",
    "# RNN: [t1]-[t2]-[t3]\n",
    "#       x1   x2   x3\n",
    "\n",
    "# Create an LSTM cell\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "# Apply dropout for regularization\n",
    "#cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.75)\n",
    "\n",
    "# Create the RNN\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "# outputs : [batch_size, max_time, cell.output_size]\n",
    "\n",
    "# Transform the output of RNN to create output values\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "# [batch_size, cell.output_size]\n",
    "model = tf.matmul(outputs, W) + b\n",
    "# [batch_size, n_classes]\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\"\"\"\n",
    "  Phase 2: Train the model\n",
    "\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    input_batch, target_batch = make_batch(training_data)\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        _, loss = sess.run([optimizer, cost],\n",
    "                           feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    print('Optimization finished')\n",
    "\n",
    "    \"\"\"\n",
    "      Make predictions\n",
    "    \"\"\"\n",
    "    seq_data = training_data # test_data\n",
    "    prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, Y), tf.float32))\n",
    "\n",
    "    input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "    predict, accuracy_val = sess.run([prediction, accuracy],\n",
    "                                     feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "    predicted = []\n",
    "    for idx, val in enumerate(seq_data):\n",
    "        last_char = vocab[predict[idx]]\n",
    "        predicted.append(val[:3] + last_char)\n",
    "\n",
    "    print('\\n=== Predictions ===')\n",
    "    print('Input:', [w[:3] + ' ' for w in seq_data])\n",
    "    print('Predicted:', predicted)\n",
    "    print('Accuracy:', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 3.438918\n",
      "Epoch: 0002 cost = 3.365723\n",
      "Epoch: 0003 cost = 3.292951\n",
      "Epoch: 0004 cost = 3.219447\n",
      "Epoch: 0005 cost = 3.144657\n",
      "Epoch: 0006 cost = 3.068118\n",
      "Epoch: 0007 cost = 2.989360\n",
      "Epoch: 0008 cost = 2.907930\n",
      "Epoch: 0009 cost = 2.823430\n",
      "Epoch: 0010 cost = 2.735563\n",
      "Epoch: 0011 cost = 2.644189\n",
      "Epoch: 0012 cost = 2.549383\n",
      "Epoch: 0013 cost = 2.451471\n",
      "Epoch: 0014 cost = 2.351079\n",
      "Epoch: 0015 cost = 2.249162\n",
      "Epoch: 0016 cost = 2.147007\n",
      "Epoch: 0017 cost = 2.046167\n",
      "Epoch: 0018 cost = 1.948310\n",
      "Epoch: 0019 cost = 1.854992\n",
      "Epoch: 0020 cost = 1.767367\n",
      "Epoch: 0021 cost = 1.685909\n",
      "Epoch: 0022 cost = 1.610252\n",
      "Epoch: 0023 cost = 1.539263\n",
      "Epoch: 0024 cost = 1.471398\n",
      "Epoch: 0025 cost = 1.405230\n",
      "Epoch: 0026 cost = 1.339834\n",
      "Epoch: 0027 cost = 1.274840\n",
      "Epoch: 0028 cost = 1.210264\n",
      "Epoch: 0029 cost = 1.146308\n",
      "Epoch: 0030 cost = 1.083337\n",
      "Epoch: 0031 cost = 1.022002\n",
      "Epoch: 0032 cost = 0.963304\n",
      "Epoch: 0033 cost = 0.908422\n",
      "Epoch: 0034 cost = 0.858272\n",
      "Epoch: 0035 cost = 0.813050\n",
      "Epoch: 0036 cost = 0.772037\n",
      "Epoch: 0037 cost = 0.733900\n",
      "Epoch: 0038 cost = 0.697380\n",
      "Epoch: 0039 cost = 0.661876\n",
      "Epoch: 0040 cost = 0.627524\n",
      "Epoch: 0041 cost = 0.594874\n",
      "Epoch: 0042 cost = 0.564460\n",
      "Epoch: 0043 cost = 0.536489\n",
      "Epoch: 0044 cost = 0.510784\n",
      "Epoch: 0045 cost = 0.486944\n",
      "Epoch: 0046 cost = 0.464572\n",
      "Epoch: 0047 cost = 0.443417\n",
      "Epoch: 0048 cost = 0.423398\n",
      "Epoch: 0049 cost = 0.404540\n",
      "Epoch: 0050 cost = 0.386911\n",
      "Epoch: 0051 cost = 0.370568\n",
      "Epoch: 0052 cost = 0.355541\n",
      "Epoch: 0053 cost = 0.341814\n",
      "Epoch: 0054 cost = 0.329295\n",
      "Epoch: 0055 cost = 0.317799\n",
      "Epoch: 0056 cost = 0.307075\n",
      "Epoch: 0057 cost = 0.296894\n",
      "Epoch: 0058 cost = 0.287148\n",
      "Epoch: 0059 cost = 0.277868\n",
      "Epoch: 0060 cost = 0.269168\n",
      "Epoch: 0061 cost = 0.261146\n",
      "Epoch: 0062 cost = 0.253817\n",
      "Epoch: 0063 cost = 0.247120\n",
      "Epoch: 0064 cost = 0.240961\n",
      "Epoch: 0065 cost = 0.235261\n",
      "Epoch: 0066 cost = 0.229986\n",
      "Epoch: 0067 cost = 0.225128\n",
      "Epoch: 0068 cost = 0.220686\n",
      "Epoch: 0069 cost = 0.216636\n",
      "Epoch: 0070 cost = 0.212928\n",
      "Epoch: 0071 cost = 0.209495\n",
      "Epoch: 0072 cost = 0.206279\n",
      "Epoch: 0073 cost = 0.203245\n",
      "Epoch: 0074 cost = 0.200383\n",
      "Epoch: 0075 cost = 0.197695\n",
      "Epoch: 0076 cost = 0.195187\n",
      "Epoch: 0077 cost = 0.192853\n",
      "Epoch: 0078 cost = 0.190681\n",
      "Epoch: 0079 cost = 0.188649\n",
      "Epoch: 0080 cost = 0.186741\n",
      "Epoch: 0081 cost = 0.184941\n",
      "Epoch: 0082 cost = 0.183240\n",
      "Epoch: 0083 cost = 0.181634\n",
      "Epoch: 0084 cost = 0.180117\n",
      "Epoch: 0085 cost = 0.178685\n",
      "Epoch: 0086 cost = 0.177332\n",
      "Epoch: 0087 cost = 0.176050\n",
      "Epoch: 0088 cost = 0.174836\n",
      "Epoch: 0089 cost = 0.173686\n",
      "Epoch: 0090 cost = 0.172598\n",
      "Epoch: 0091 cost = 0.171569\n",
      "Epoch: 0092 cost = 0.170596\n",
      "Epoch: 0093 cost = 0.169677\n",
      "Epoch: 0094 cost = 0.168805\n",
      "Epoch: 0095 cost = 0.167975\n",
      "Epoch: 0096 cost = 0.167183\n",
      "Epoch: 0097 cost = 0.166425\n",
      "Epoch: 0098 cost = 0.165699\n",
      "Epoch: 0099 cost = 0.165004\n",
      "Epoch: 0100 cost = 0.164340\n",
      "Optimization finished\n",
      "\n",
      "=== Predictions ===\n",
      "Input: ['wo_d', 'wo_d', 'de_p', 'di_e', 'co_d', 'co_l', 'lo_d', 'lo_e', 'ki_s', 'ki_d']\n",
      "Predicted: ['wood', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
      "Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  Autocompletion of the last character of words\n",
    "  Given the first three letters of a four-letters word, learn to predict the last letter\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vocab = ['a', 'b', 'c', 'd', 'e', 'f', 'g',\n",
    "         'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "         'o', 'p', 'q', 'r', 's', 't', 'u',\n",
    "         'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# index array of characters in vocab\n",
    "v_map = {n: i for i, n in enumerate(vocab)}\n",
    "v_len = len(v_map)\n",
    "\n",
    "# training data (character sequences)\n",
    "# wor -> X, d -> Y\n",
    "# woo -> X, d -> Y\n",
    "training_data = ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
    "test_data = ['wood', 'deep', 'cold', 'load', 'love', 'dear', 'dove', 'cell', 'life', 'keep']\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        # Indices of the first three alphabets of the words\n",
    "        # [22, 14, 17] [22, 14, 14] [3, 4, 4] [3, 8, 21] ...\n",
    "        input = [v_map[seq[0]], v_map[seq[1]], v_map[seq[-1]]]\n",
    "        # Indices of the last alphabet of the words\n",
    "        # 3, 3, 15, 4, 3 ...\n",
    "        target = v_map[seq[-2]]\n",
    "\n",
    "        # One-hot encoding of the inputs into the sequences of 26-dimensional vectors\n",
    "        # [0, 1, 2] ==>\n",
    "        # [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "        #  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "        #  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
    "        input_batch.append(np.eye(v_len)[input])\n",
    "\n",
    "        # We don't apply one-hot encoding for the output,\n",
    "        # since we'll use sparse_softmax_cross_entropy_with_logits\n",
    "        # as our loss function\n",
    "        target_batch.append(target)\n",
    "\n",
    "    return input_batch, target_batch\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_hidden = 10\n",
    "total_epoch = 100\n",
    "n_step = 3 # the length of the input sequence\n",
    "n_input = n_class = v_len # the size of each input\n",
    "\n",
    "\"\"\"\n",
    "  Phase 1: Create the computation graph\n",
    "\"\"\"\n",
    "X = tf.placeholder(tf.float32, [None, n_step, n_input])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "#                output (pred. of forth letter)\n",
    "#                 | (W,b)\n",
    "#                outputs (hidden)\n",
    "#       |    |    |\n",
    "# RNN: [t1]-[t2]-[t3]\n",
    "#       x1   x2   x3\n",
    "\n",
    "# Create an LSTM cell\n",
    "cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden)\n",
    "# Apply dropout for regularization\n",
    "#cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.75)\n",
    "\n",
    "# Create the RNN\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "# outputs : [batch_size, max_time, cell.output_size]\n",
    "\n",
    "# Transform the output of RNN to create output values\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "outputs = outputs[-1]\n",
    "# [batch_size, cell.output_size]\n",
    "model = tf.matmul(outputs, W) + b\n",
    "# [batch_size, n_classes]\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\"\"\"\n",
    "  Phase 2: Train the model\n",
    "\"\"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    input_batch, target_batch = make_batch(training_data)\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        _, loss = sess.run([optimizer, cost],\n",
    "                           feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "        print('Epoch:', '%04d' % (epoch + 1),\n",
    "              'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    print('Optimization finished')\n",
    "\n",
    "    \"\"\"\n",
    "      Make predictions\n",
    "    \"\"\"\n",
    "    seq_data = training_data # test_data\n",
    "    prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, Y), tf.float32))\n",
    "\n",
    "    input_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "    predict, accuracy_val = sess.run([prediction, accuracy],\n",
    "                                     feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "    predicted = []\n",
    "    for idx, val in enumerate(seq_data):\n",
    "        middle_char = vocab[predict[idx]]\n",
    "        predicted.append(val[:2] + middle_char + val[-1])\n",
    "\n",
    "    print('\\n=== Predictions ===')\n",
    "    print('Input:', [\"{}_{}\".format(w[:2], w[-1]) for w in seq_data])\n",
    "    print('Predicted:', predicted)\n",
    "    print('Accuracy:', accuracy_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
