{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Project_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2921
    },
    "colab_type": "code",
    "id": "GtvZ4nnBA5kf",
    "outputId": "77043b4e-7721-4e46-c331-11d3131d4d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Conv 1 =  Tensor(\"Relu_2:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Conv 1 =  Tensor(\"MaxPool_1:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "Iter 1280, Minibatch Loss= 920.308594, Training Accuracy= 0.36719\n",
      "Iter 2560, Minibatch Loss= 550.487000, Training Accuracy= 0.57812\n",
      "Iter 3840, Minibatch Loss= 308.081482, Training Accuracy= 0.76562\n",
      "Iter 5120, Minibatch Loss= 311.360413, Training Accuracy= 0.79688\n",
      "Iter 6400, Minibatch Loss= 188.203400, Training Accuracy= 0.78125\n",
      "Iter 7680, Minibatch Loss= 91.991409, Training Accuracy= 0.88281\n",
      "Iter 8960, Minibatch Loss= 169.586243, Training Accuracy= 0.82031\n",
      "Iter 10240, Minibatch Loss= 179.926605, Training Accuracy= 0.87500\n",
      "Iter 11520, Minibatch Loss= 107.111649, Training Accuracy= 0.89844\n",
      "Iter 12800, Minibatch Loss= 127.493385, Training Accuracy= 0.85938\n",
      "Iter 14080, Minibatch Loss= 144.384735, Training Accuracy= 0.90625\n",
      "Iter 15360, Minibatch Loss= 140.383698, Training Accuracy= 0.90625\n",
      "Iter 16640, Minibatch Loss= 134.033173, Training Accuracy= 0.86719\n",
      "Iter 17920, Minibatch Loss= 52.099056, Training Accuracy= 0.92969\n",
      "Iter 19200, Minibatch Loss= 94.466156, Training Accuracy= 0.88281\n",
      "Iter 20480, Minibatch Loss= 140.913422, Training Accuracy= 0.89062\n",
      "Iter 21760, Minibatch Loss= 50.040207, Training Accuracy= 0.94531\n",
      "Iter 23040, Minibatch Loss= 117.665123, Training Accuracy= 0.91406\n",
      "Iter 24320, Minibatch Loss= 113.738350, Training Accuracy= 0.91406\n",
      "Iter 25600, Minibatch Loss= 31.115429, Training Accuracy= 0.93750\n",
      "Iter 26880, Minibatch Loss= 95.508278, Training Accuracy= 0.91406\n",
      "Iter 28160, Minibatch Loss= 106.426941, Training Accuracy= 0.88281\n",
      "Iter 29440, Minibatch Loss= 76.574509, Training Accuracy= 0.94531\n",
      "Iter 30720, Minibatch Loss= 82.159454, Training Accuracy= 0.95312\n",
      "Iter 32000, Minibatch Loss= 99.790436, Training Accuracy= 0.90625\n",
      "Iter 33280, Minibatch Loss= 59.346001, Training Accuracy= 0.93750\n",
      "Iter 34560, Minibatch Loss= 140.634491, Training Accuracy= 0.91406\n",
      "Iter 35840, Minibatch Loss= 108.039520, Training Accuracy= 0.92188\n",
      "Iter 37120, Minibatch Loss= 100.336075, Training Accuracy= 0.87500\n",
      "Iter 38400, Minibatch Loss= 81.932053, Training Accuracy= 0.90625\n",
      "Iter 39680, Minibatch Loss= 66.378067, Training Accuracy= 0.91406\n",
      "Iter 40960, Minibatch Loss= 60.787392, Training Accuracy= 0.92969\n",
      "Iter 42240, Minibatch Loss= 18.059078, Training Accuracy= 0.92969\n",
      "Iter 43520, Minibatch Loss= 66.011589, Training Accuracy= 0.87500\n",
      "Iter 44800, Minibatch Loss= 28.247530, Training Accuracy= 0.94531\n",
      "Iter 46080, Minibatch Loss= 70.763008, Training Accuracy= 0.94531\n",
      "Iter 47360, Minibatch Loss= 53.228710, Training Accuracy= 0.94531\n",
      "Iter 48640, Minibatch Loss= 39.511559, Training Accuracy= 0.93750\n",
      "Iter 49920, Minibatch Loss= 92.666779, Training Accuracy= 0.92969\n",
      "Iter 51200, Minibatch Loss= 71.397499, Training Accuracy= 0.92188\n",
      "Iter 52480, Minibatch Loss= 44.435768, Training Accuracy= 0.95312\n",
      "Iter 53760, Minibatch Loss= 60.518131, Training Accuracy= 0.92969\n",
      "Iter 55040, Minibatch Loss= 48.039143, Training Accuracy= 0.95312\n",
      "Iter 56320, Minibatch Loss= 37.894592, Training Accuracy= 0.94531\n",
      "Iter 57600, Minibatch Loss= 45.258327, Training Accuracy= 0.92188\n",
      "Iter 58880, Minibatch Loss= 75.662544, Training Accuracy= 0.91406\n",
      "Iter 60160, Minibatch Loss= 37.007118, Training Accuracy= 0.96094\n",
      "Iter 61440, Minibatch Loss= 13.691716, Training Accuracy= 0.96094\n",
      "Iter 62720, Minibatch Loss= 21.657661, Training Accuracy= 0.96094\n",
      "Iter 64000, Minibatch Loss= 71.524010, Training Accuracy= 0.94531\n",
      "Iter 65280, Minibatch Loss= 19.623674, Training Accuracy= 0.96094\n",
      "Iter 66560, Minibatch Loss= 118.116707, Training Accuracy= 0.91406\n",
      "Iter 67840, Minibatch Loss= 6.363690, Training Accuracy= 0.97656\n",
      "Iter 69120, Minibatch Loss= 34.021881, Training Accuracy= 0.95312\n",
      "Iter 70400, Minibatch Loss= 17.967247, Training Accuracy= 0.97656\n",
      "Iter 71680, Minibatch Loss= 21.111246, Training Accuracy= 0.96875\n",
      "Iter 72960, Minibatch Loss= 37.606407, Training Accuracy= 0.94531\n",
      "Iter 74240, Minibatch Loss= 71.592102, Training Accuracy= 0.95312\n",
      "Iter 75520, Minibatch Loss= 32.756950, Training Accuracy= 0.92969\n",
      "Iter 76800, Minibatch Loss= 43.937794, Training Accuracy= 0.96094\n",
      "Iter 78080, Minibatch Loss= 13.157248, Training Accuracy= 0.97656\n",
      "Iter 79360, Minibatch Loss= 26.970419, Training Accuracy= 0.95312\n",
      "Iter 80640, Minibatch Loss= 22.064953, Training Accuracy= 0.95312\n",
      "Iter 81920, Minibatch Loss= 35.379227, Training Accuracy= 0.95312\n",
      "Iter 83200, Minibatch Loss= 39.892265, Training Accuracy= 0.92969\n",
      "Iter 84480, Minibatch Loss= 15.805943, Training Accuracy= 0.95312\n",
      "Iter 85760, Minibatch Loss= 51.053196, Training Accuracy= 0.94531\n",
      "Iter 87040, Minibatch Loss= 17.535860, Training Accuracy= 0.97656\n",
      "Iter 88320, Minibatch Loss= 20.663528, Training Accuracy= 0.95312\n",
      "Iter 89600, Minibatch Loss= 10.662054, Training Accuracy= 0.96875\n",
      "Iter 90880, Minibatch Loss= 27.487366, Training Accuracy= 0.95312\n",
      "Iter 92160, Minibatch Loss= 35.287495, Training Accuracy= 0.92969\n",
      "Iter 93440, Minibatch Loss= 50.154015, Training Accuracy= 0.92188\n",
      "Iter 94720, Minibatch Loss= 18.150059, Training Accuracy= 0.96875\n",
      "Iter 96000, Minibatch Loss= 23.820532, Training Accuracy= 0.94531\n",
      "Iter 97280, Minibatch Loss= 53.296677, Training Accuracy= 0.94531\n",
      "Iter 98560, Minibatch Loss= 54.612701, Training Accuracy= 0.92969\n",
      "Iter 99840, Minibatch Loss= 19.514683, Training Accuracy= 0.94531\n",
      "Iter 101120, Minibatch Loss= 23.061939, Training Accuracy= 0.96094\n",
      "Iter 102400, Minibatch Loss= 46.760963, Training Accuracy= 0.93750\n",
      "Iter 103680, Minibatch Loss= 36.597572, Training Accuracy= 0.94531\n",
      "Iter 104960, Minibatch Loss= 20.316442, Training Accuracy= 0.95312\n",
      "Iter 106240, Minibatch Loss= 16.240158, Training Accuracy= 0.97656\n",
      "Iter 107520, Minibatch Loss= 13.102427, Training Accuracy= 0.94531\n",
      "Iter 108800, Minibatch Loss= 18.329723, Training Accuracy= 0.96094\n",
      "Iter 110080, Minibatch Loss= 40.930782, Training Accuracy= 0.90625\n",
      "Iter 111360, Minibatch Loss= 7.369382, Training Accuracy= 0.97656\n",
      "Iter 112640, Minibatch Loss= 7.368252, Training Accuracy= 0.96094\n",
      "Iter 113920, Minibatch Loss= 11.908057, Training Accuracy= 0.96875\n",
      "Iter 115200, Minibatch Loss= 13.109626, Training Accuracy= 0.97656\n",
      "Iter 116480, Minibatch Loss= 21.021111, Training Accuracy= 0.93750\n",
      "Iter 117760, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 119040, Minibatch Loss= 17.490837, Training Accuracy= 0.94531\n",
      "Iter 120320, Minibatch Loss= 36.385033, Training Accuracy= 0.93750\n",
      "Iter 121600, Minibatch Loss= 30.321537, Training Accuracy= 0.93750\n",
      "Iter 122880, Minibatch Loss= 19.522209, Training Accuracy= 0.94531\n",
      "Iter 124160, Minibatch Loss= 4.190234, Training Accuracy= 0.98438\n",
      "Iter 125440, Minibatch Loss= 28.172089, Training Accuracy= 0.92969\n",
      "Iter 126720, Minibatch Loss= 23.252958, Training Accuracy= 0.96094\n",
      "Iter 128000, Minibatch Loss= 12.379713, Training Accuracy= 0.94531\n",
      "Iter 129280, Minibatch Loss= 18.532888, Training Accuracy= 0.96094\n",
      "Iter 130560, Minibatch Loss= 11.397752, Training Accuracy= 0.94531\n",
      "Iter 131840, Minibatch Loss= 21.388290, Training Accuracy= 0.95312\n",
      "Iter 133120, Minibatch Loss= 9.856493, Training Accuracy= 0.98438\n",
      "Iter 134400, Minibatch Loss= 9.683426, Training Accuracy= 0.97656\n",
      "Iter 135680, Minibatch Loss= 43.234978, Training Accuracy= 0.96094\n",
      "Iter 136960, Minibatch Loss= 12.159592, Training Accuracy= 0.96875\n",
      "Iter 138240, Minibatch Loss= 22.261070, Training Accuracy= 0.96875\n",
      "Iter 139520, Minibatch Loss= 35.777092, Training Accuracy= 0.92969\n",
      "Iter 140800, Minibatch Loss= 21.953651, Training Accuracy= 0.97656\n",
      "Iter 142080, Minibatch Loss= 12.096503, Training Accuracy= 0.95312\n",
      "Iter 143360, Minibatch Loss= 12.599445, Training Accuracy= 0.96094\n",
      "Iter 144640, Minibatch Loss= 3.790724, Training Accuracy= 0.97656\n",
      "Iter 145920, Minibatch Loss= 13.583252, Training Accuracy= 0.95312\n",
      "Iter 147200, Minibatch Loss= 17.536049, Training Accuracy= 0.95312\n",
      "Iter 148480, Minibatch Loss= 22.260403, Training Accuracy= 0.96094\n",
      "Iter 149760, Minibatch Loss= 11.288902, Training Accuracy= 0.97656\n",
      "Iter 151040, Minibatch Loss= 8.634016, Training Accuracy= 0.96875\n",
      "Iter 152320, Minibatch Loss= 23.858395, Training Accuracy= 0.95312\n",
      "Iter 153600, Minibatch Loss= 30.046436, Training Accuracy= 0.93750\n",
      "Iter 154880, Minibatch Loss= 13.778240, Training Accuracy= 0.97656\n",
      "Iter 156160, Minibatch Loss= 17.494730, Training Accuracy= 0.96094\n",
      "Iter 157440, Minibatch Loss= 17.702131, Training Accuracy= 0.95312\n",
      "Iter 158720, Minibatch Loss= 16.472031, Training Accuracy= 0.96875\n",
      "Iter 160000, Minibatch Loss= 15.996449, Training Accuracy= 0.98438\n",
      "Iter 161280, Minibatch Loss= 20.404921, Training Accuracy= 0.94531\n",
      "Iter 162560, Minibatch Loss= 28.343504, Training Accuracy= 0.94531\n",
      "Iter 163840, Minibatch Loss= 6.999977, Training Accuracy= 0.97656\n",
      "Iter 165120, Minibatch Loss= 7.337893, Training Accuracy= 0.97656\n",
      "Iter 166400, Minibatch Loss= 7.693323, Training Accuracy= 0.96875\n",
      "Iter 167680, Minibatch Loss= 23.476982, Training Accuracy= 0.95312\n",
      "Iter 168960, Minibatch Loss= 5.656531, Training Accuracy= 0.97656\n",
      "Iter 170240, Minibatch Loss= 16.184921, Training Accuracy= 0.96094\n",
      "Iter 171520, Minibatch Loss= 0.859931, Training Accuracy= 0.98438\n",
      "Iter 172800, Minibatch Loss= 10.323277, Training Accuracy= 0.95312\n",
      "Iter 174080, Minibatch Loss= 13.084929, Training Accuracy= 0.96875\n",
      "Iter 175360, Minibatch Loss= 5.760456, Training Accuracy= 0.97656\n",
      "Iter 176640, Minibatch Loss= 3.281452, Training Accuracy= 0.97656\n",
      "Iter 177920, Minibatch Loss= 9.024288, Training Accuracy= 0.95312\n",
      "Iter 179200, Minibatch Loss= 5.043134, Training Accuracy= 0.96875\n",
      "Iter 180480, Minibatch Loss= 21.184593, Training Accuracy= 0.96094\n",
      "Iter 181760, Minibatch Loss= 2.277784, Training Accuracy= 0.99219\n",
      "Iter 183040, Minibatch Loss= 5.606327, Training Accuracy= 0.96875\n",
      "Iter 184320, Minibatch Loss= 2.996469, Training Accuracy= 0.98438\n",
      "Iter 185600, Minibatch Loss= 6.589676, Training Accuracy= 0.95312\n",
      "Iter 186880, Minibatch Loss= 0.782156, Training Accuracy= 0.99219\n",
      "Iter 188160, Minibatch Loss= 1.003601, Training Accuracy= 0.97656\n",
      "Iter 189440, Minibatch Loss= 12.186762, Training Accuracy= 0.97656\n",
      "Iter 190720, Minibatch Loss= 25.633261, Training Accuracy= 0.95312\n",
      "Iter 192000, Minibatch Loss= 13.888302, Training Accuracy= 0.95312\n",
      "Iter 193280, Minibatch Loss= 8.055140, Training Accuracy= 0.96875\n",
      "Iter 194560, Minibatch Loss= 0.000028, Training Accuracy= 1.00000\n",
      "Iter 195840, Minibatch Loss= 9.135677, Training Accuracy= 0.96094\n",
      "Iter 197120, Minibatch Loss= 16.279350, Training Accuracy= 0.96094\n",
      "Iter 198400, Minibatch Loss= 4.012445, Training Accuracy= 0.98438\n",
      "Iter 199680, Minibatch Loss= 12.742594, Training Accuracy= 0.96875\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.98046875\n",
      "Accuracy 0.9633\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CNN to classify MNIST handwritten digits\n",
    "'''\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read in MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/mnist\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # input image shape = 28*28 grey scale\n",
    "n_classes = 10 # 10 classes (0-9 digits)\n",
    "dropout = 0.75 # probability to keep units during dropout\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n",
    "\n",
    "# Wrappers\n",
    "def reshape(x, xdim, ydim):\n",
    "    return tf.reshape(x, shape=[-1, xdim, ydim, 1])\n",
    "\n",
    "def conv2d(x, W, b, stride=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    return tf.nn.relu(x + b)\n",
    "\n",
    "def maxpool2d(x, size=2, stride=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, size, size, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = reshape(x, 28, 28)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    print(\"Conv 1 = \", conv1)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, size=2, stride=2)\n",
    "    print(\"Conv 1 = \", conv1)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv1, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # fully connected, ? inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([14 * 14 * 32, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    # 'bc2': tf.Variable(tf.random_normal([?])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            l, acc = sess.run([loss, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(l) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    \n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256], keep_prob: 1.}))\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred, axis=1), tf.argmax(y, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_pred = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={x: X_batch, y:Y_batch, keep_prob: 1.})\n",
    "        total_correct_pred += accuracy_batch\n",
    "\n",
    "    print('Accuracy {0}'.format(total_correct_pred/mnist.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2956
    },
    "colab_type": "code",
    "id": "9AxXYkSHA8pQ",
    "outputId": "52ef8f4c-bdee-4b98-f670-55322e9d1ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Conv 1 =  Tensor(\"Relu_9:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Conv 1 =  Tensor(\"MaxPool_6:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "Conv 2 =  Tensor(\"Relu_10:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
      "Conv 2 =  Tensor(\"MaxPool_7:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "Iter 1280, Minibatch Loss= 30333.636719, Training Accuracy= 0.20312\n",
      "Iter 2560, Minibatch Loss= 9934.336914, Training Accuracy= 0.54688\n",
      "Iter 3840, Minibatch Loss= 5916.205078, Training Accuracy= 0.69531\n",
      "Iter 5120, Minibatch Loss= 5468.480469, Training Accuracy= 0.71875\n",
      "Iter 6400, Minibatch Loss= 5385.360352, Training Accuracy= 0.72656\n",
      "Iter 7680, Minibatch Loss= 2076.635010, Training Accuracy= 0.83594\n",
      "Iter 8960, Minibatch Loss= 2217.161621, Training Accuracy= 0.84375\n",
      "Iter 10240, Minibatch Loss= 3669.830811, Training Accuracy= 0.81250\n",
      "Iter 11520, Minibatch Loss= 3133.350098, Training Accuracy= 0.88281\n",
      "Iter 12800, Minibatch Loss= 1362.285889, Training Accuracy= 0.92969\n",
      "Iter 14080, Minibatch Loss= 2523.880859, Training Accuracy= 0.88281\n",
      "Iter 15360, Minibatch Loss= 1313.603760, Training Accuracy= 0.92188\n",
      "Iter 16640, Minibatch Loss= 1648.074219, Training Accuracy= 0.86719\n",
      "Iter 17920, Minibatch Loss= 1794.004883, Training Accuracy= 0.89062\n",
      "Iter 19200, Minibatch Loss= 1340.096802, Training Accuracy= 0.89844\n",
      "Iter 20480, Minibatch Loss= 1409.191162, Training Accuracy= 0.89062\n",
      "Iter 21760, Minibatch Loss= 1189.252319, Training Accuracy= 0.92188\n",
      "Iter 23040, Minibatch Loss= 2255.738281, Training Accuracy= 0.87500\n",
      "Iter 24320, Minibatch Loss= 2070.970947, Training Accuracy= 0.91406\n",
      "Iter 25600, Minibatch Loss= 1566.439697, Training Accuracy= 0.88281\n",
      "Iter 26880, Minibatch Loss= 688.771362, Training Accuracy= 0.96094\n",
      "Iter 28160, Minibatch Loss= 2301.657227, Training Accuracy= 0.87500\n",
      "Iter 29440, Minibatch Loss= 1054.961304, Training Accuracy= 0.92969\n",
      "Iter 30720, Minibatch Loss= 1786.828979, Training Accuracy= 0.91406\n",
      "Iter 32000, Minibatch Loss= 1311.893066, Training Accuracy= 0.92188\n",
      "Iter 33280, Minibatch Loss= 725.241394, Training Accuracy= 0.95312\n",
      "Iter 34560, Minibatch Loss= 1404.230103, Training Accuracy= 0.89062\n",
      "Iter 35840, Minibatch Loss= 652.854736, Training Accuracy= 0.94531\n",
      "Iter 37120, Minibatch Loss= 1256.509399, Training Accuracy= 0.94531\n",
      "Iter 38400, Minibatch Loss= 797.701233, Training Accuracy= 0.92969\n",
      "Iter 39680, Minibatch Loss= 1398.902344, Training Accuracy= 0.94531\n",
      "Iter 40960, Minibatch Loss= 1614.584229, Training Accuracy= 0.89062\n",
      "Iter 42240, Minibatch Loss= 1243.821045, Training Accuracy= 0.93750\n",
      "Iter 43520, Minibatch Loss= 911.642090, Training Accuracy= 0.92188\n",
      "Iter 44800, Minibatch Loss= 1201.932861, Training Accuracy= 0.93750\n",
      "Iter 46080, Minibatch Loss= 214.813919, Training Accuracy= 0.95312\n",
      "Iter 47360, Minibatch Loss= 857.417480, Training Accuracy= 0.94531\n",
      "Iter 48640, Minibatch Loss= 690.293213, Training Accuracy= 0.95312\n",
      "Iter 49920, Minibatch Loss= 969.258911, Training Accuracy= 0.94531\n",
      "Iter 51200, Minibatch Loss= 934.805237, Training Accuracy= 0.95312\n",
      "Iter 52480, Minibatch Loss= 258.787720, Training Accuracy= 0.97656\n",
      "Iter 53760, Minibatch Loss= 369.563507, Training Accuracy= 0.97656\n",
      "Iter 55040, Minibatch Loss= 395.673279, Training Accuracy= 0.97656\n",
      "Iter 56320, Minibatch Loss= 1679.560425, Training Accuracy= 0.90625\n",
      "Iter 57600, Minibatch Loss= 1164.572754, Training Accuracy= 0.91406\n",
      "Iter 58880, Minibatch Loss= 910.930908, Training Accuracy= 0.96094\n",
      "Iter 60160, Minibatch Loss= 396.732849, Training Accuracy= 0.96094\n",
      "Iter 61440, Minibatch Loss= 381.176422, Training Accuracy= 0.96094\n",
      "Iter 62720, Minibatch Loss= 393.626648, Training Accuracy= 0.95312\n",
      "Iter 64000, Minibatch Loss= 603.958496, Training Accuracy= 0.93750\n",
      "Iter 65280, Minibatch Loss= 747.590576, Training Accuracy= 0.94531\n",
      "Iter 66560, Minibatch Loss= 746.910950, Training Accuracy= 0.94531\n",
      "Iter 67840, Minibatch Loss= 488.816162, Training Accuracy= 0.96875\n",
      "Iter 69120, Minibatch Loss= 297.033417, Training Accuracy= 0.94531\n",
      "Iter 70400, Minibatch Loss= 807.497803, Training Accuracy= 0.92188\n",
      "Iter 71680, Minibatch Loss= 276.263397, Training Accuracy= 0.96094\n",
      "Iter 72960, Minibatch Loss= 598.385376, Training Accuracy= 0.93750\n",
      "Iter 74240, Minibatch Loss= 294.374573, Training Accuracy= 0.96875\n",
      "Iter 75520, Minibatch Loss= 549.738403, Training Accuracy= 0.94531\n",
      "Iter 76800, Minibatch Loss= 62.274841, Training Accuracy= 0.98438\n",
      "Iter 78080, Minibatch Loss= 1329.724609, Training Accuracy= 0.89062\n",
      "Iter 79360, Minibatch Loss= 407.702148, Training Accuracy= 0.96875\n",
      "Iter 80640, Minibatch Loss= 257.601562, Training Accuracy= 0.95312\n",
      "Iter 81920, Minibatch Loss= 462.388000, Training Accuracy= 0.94531\n",
      "Iter 83200, Minibatch Loss= 437.850311, Training Accuracy= 0.96875\n",
      "Iter 84480, Minibatch Loss= 911.328552, Training Accuracy= 0.94531\n",
      "Iter 85760, Minibatch Loss= 187.848785, Training Accuracy= 0.96875\n",
      "Iter 87040, Minibatch Loss= 310.355103, Training Accuracy= 0.96875\n",
      "Iter 88320, Minibatch Loss= 395.529816, Training Accuracy= 0.96875\n",
      "Iter 89600, Minibatch Loss= 766.797913, Training Accuracy= 0.95312\n",
      "Iter 90880, Minibatch Loss= 460.020782, Training Accuracy= 0.94531\n",
      "Iter 92160, Minibatch Loss= 502.107544, Training Accuracy= 0.95312\n",
      "Iter 93440, Minibatch Loss= 840.302612, Training Accuracy= 0.96094\n",
      "Iter 94720, Minibatch Loss= 303.450317, Training Accuracy= 0.95312\n",
      "Iter 96000, Minibatch Loss= 347.175293, Training Accuracy= 0.96875\n",
      "Iter 97280, Minibatch Loss= 378.882629, Training Accuracy= 0.95312\n",
      "Iter 98560, Minibatch Loss= 176.232452, Training Accuracy= 0.96875\n",
      "Iter 99840, Minibatch Loss= 209.314911, Training Accuracy= 0.98438\n",
      "Iter 101120, Minibatch Loss= 109.494957, Training Accuracy= 0.97656\n",
      "Iter 102400, Minibatch Loss= 307.354095, Training Accuracy= 0.96094\n",
      "Iter 103680, Minibatch Loss= 520.951904, Training Accuracy= 0.97656\n",
      "Iter 104960, Minibatch Loss= 359.210876, Training Accuracy= 0.95312\n",
      "Iter 106240, Minibatch Loss= 275.095825, Training Accuracy= 0.95312\n",
      "Iter 107520, Minibatch Loss= 400.768921, Training Accuracy= 0.96094\n",
      "Iter 108800, Minibatch Loss= 169.073761, Training Accuracy= 0.98438\n",
      "Iter 110080, Minibatch Loss= 249.409348, Training Accuracy= 0.93750\n",
      "Iter 111360, Minibatch Loss= 242.409256, Training Accuracy= 0.96875\n",
      "Iter 112640, Minibatch Loss= 119.232719, Training Accuracy= 0.99219\n",
      "Iter 113920, Minibatch Loss= 70.653427, Training Accuracy= 0.99219\n",
      "Iter 115200, Minibatch Loss= 31.226242, Training Accuracy= 0.98438\n",
      "Iter 116480, Minibatch Loss= 227.955276, Training Accuracy= 0.98438\n",
      "Iter 117760, Minibatch Loss= 304.254089, Training Accuracy= 0.95312\n",
      "Iter 119040, Minibatch Loss= 563.681885, Training Accuracy= 0.96875\n",
      "Iter 120320, Minibatch Loss= 167.507492, Training Accuracy= 0.96094\n",
      "Iter 121600, Minibatch Loss= 265.911133, Training Accuracy= 0.96094\n",
      "Iter 122880, Minibatch Loss= 200.065399, Training Accuracy= 0.96094\n",
      "Iter 124160, Minibatch Loss= 153.967758, Training Accuracy= 0.98438\n",
      "Iter 125440, Minibatch Loss= 218.982010, Training Accuracy= 0.96094\n",
      "Iter 126720, Minibatch Loss= 212.184677, Training Accuracy= 0.96875\n",
      "Iter 128000, Minibatch Loss= 168.611542, Training Accuracy= 0.97656\n",
      "Iter 129280, Minibatch Loss= 56.067703, Training Accuracy= 0.98438\n",
      "Iter 130560, Minibatch Loss= 118.728241, Training Accuracy= 0.96094\n",
      "Iter 131840, Minibatch Loss= 237.675018, Training Accuracy= 0.97656\n",
      "Iter 133120, Minibatch Loss= 468.642822, Training Accuracy= 0.96094\n",
      "Iter 134400, Minibatch Loss= 92.006851, Training Accuracy= 0.96875\n",
      "Iter 135680, Minibatch Loss= 233.205734, Training Accuracy= 0.97656\n",
      "Iter 136960, Minibatch Loss= 127.816162, Training Accuracy= 0.98438\n",
      "Iter 138240, Minibatch Loss= 346.985474, Training Accuracy= 0.96875\n",
      "Iter 139520, Minibatch Loss= 17.000351, Training Accuracy= 0.98438\n",
      "Iter 140800, Minibatch Loss= 273.307251, Training Accuracy= 0.97656\n",
      "Iter 142080, Minibatch Loss= 129.849213, Training Accuracy= 0.97656\n",
      "Iter 143360, Minibatch Loss= 245.018417, Training Accuracy= 0.92188\n",
      "Iter 144640, Minibatch Loss= 127.222122, Training Accuracy= 0.97656\n",
      "Iter 145920, Minibatch Loss= 113.761871, Training Accuracy= 0.99219\n",
      "Iter 147200, Minibatch Loss= 56.777596, Training Accuracy= 0.97656\n",
      "Iter 148480, Minibatch Loss= 356.612366, Training Accuracy= 0.95312\n",
      "Iter 149760, Minibatch Loss= 219.956665, Training Accuracy= 0.96094\n",
      "Iter 151040, Minibatch Loss= 234.206650, Training Accuracy= 0.97656\n",
      "Iter 152320, Minibatch Loss= 520.390991, Training Accuracy= 0.94531\n",
      "Iter 153600, Minibatch Loss= 231.205338, Training Accuracy= 0.92969\n",
      "Iter 154880, Minibatch Loss= 370.904724, Training Accuracy= 0.95312\n",
      "Iter 156160, Minibatch Loss= 158.203812, Training Accuracy= 0.97656\n",
      "Iter 157440, Minibatch Loss= 52.173431, Training Accuracy= 0.97656\n",
      "Iter 158720, Minibatch Loss= 373.606506, Training Accuracy= 0.97656\n",
      "Iter 160000, Minibatch Loss= 11.532898, Training Accuracy= 0.99219\n",
      "Iter 161280, Minibatch Loss= 96.094086, Training Accuracy= 0.99219\n",
      "Iter 162560, Minibatch Loss= 128.297653, Training Accuracy= 0.98438\n",
      "Iter 163840, Minibatch Loss= 325.322021, Training Accuracy= 0.94531\n",
      "Iter 165120, Minibatch Loss= 143.900406, Training Accuracy= 0.96875\n",
      "Iter 166400, Minibatch Loss= 89.771744, Training Accuracy= 0.99219\n",
      "Iter 167680, Minibatch Loss= 60.928123, Training Accuracy= 0.98438\n",
      "Iter 168960, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 170240, Minibatch Loss= 725.182556, Training Accuracy= 0.92969\n",
      "Iter 171520, Minibatch Loss= 499.024384, Training Accuracy= 0.92969\n",
      "Iter 172800, Minibatch Loss= 9.736023, Training Accuracy= 0.97656\n",
      "Iter 174080, Minibatch Loss= 234.043518, Training Accuracy= 0.97656\n",
      "Iter 175360, Minibatch Loss= 165.604889, Training Accuracy= 0.96875\n",
      "Iter 176640, Minibatch Loss= 78.632744, Training Accuracy= 0.98438\n",
      "Iter 177920, Minibatch Loss= 186.578796, Training Accuracy= 0.98438\n",
      "Iter 179200, Minibatch Loss= 108.891502, Training Accuracy= 0.98438\n",
      "Iter 180480, Minibatch Loss= 151.458954, Training Accuracy= 0.97656\n",
      "Iter 181760, Minibatch Loss= 197.814087, Training Accuracy= 0.96875\n",
      "Iter 183040, Minibatch Loss= 66.424995, Training Accuracy= 0.98438\n",
      "Iter 184320, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 185600, Minibatch Loss= 361.849121, Training Accuracy= 0.98438\n",
      "Iter 186880, Minibatch Loss= 140.905701, Training Accuracy= 0.98438\n",
      "Iter 188160, Minibatch Loss= 310.676178, Training Accuracy= 0.96094\n",
      "Iter 189440, Minibatch Loss= 121.140366, Training Accuracy= 0.98438\n",
      "Iter 190720, Minibatch Loss= 251.400223, Training Accuracy= 0.94531\n",
      "Iter 192000, Minibatch Loss= 206.431229, Training Accuracy= 0.96875\n",
      "Iter 193280, Minibatch Loss= 47.546455, Training Accuracy= 0.96875\n",
      "Iter 194560, Minibatch Loss= 32.015709, Training Accuracy= 0.99219\n",
      "Iter 195840, Minibatch Loss= 41.495697, Training Accuracy= 0.97656\n",
      "Iter 197120, Minibatch Loss= 58.645355, Training Accuracy= 0.97656\n",
      "Iter 198400, Minibatch Loss= 125.642288, Training Accuracy= 0.98438\n",
      "Iter 199680, Minibatch Loss= 119.635147, Training Accuracy= 0.97656\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9765625\n",
      "Accuracy 0.9711\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CNN to classify MNIST handwritten digits\n",
    "'''\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read in MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/mnist\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # input image shape = 28*28 grey scale\n",
    "n_classes = 10 # 10 classes (0-9 digits)\n",
    "dropout = 0.75 # probability to keep units during dropout\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)\n",
    "\n",
    "# Wrappers\n",
    "def reshape(x, xdim, ydim):\n",
    "    return tf.reshape(x, shape=[-1, xdim, ydim, 1])\n",
    "\n",
    "def conv2d(x, W, b, stride=1):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    return tf.nn.relu(x + b)\n",
    "\n",
    "def maxpool2d(x, size=2, stride=2):\n",
    "    return tf.nn.max_pool(x, ksize=[1, size, size, 1], strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = reshape(x, 28, 28)\n",
    "\n",
    "    # Convolution Layer 1\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    print(\"Conv 1 = \", conv1)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, size=2, stride=2)\n",
    "    print(\"Conv 1 = \", conv1)\n",
    "\n",
    "    # Convolution Layer 2\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    print(\"Conv 2 = \", conv2)\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, size=2, stride=2)\n",
    "    print(\"Conv 2 = \", conv2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 1 input, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, ? inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7 * 7 * 64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            l, acc = sess.run([loss, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(l) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    \n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256], keep_prob: 1.}))\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(pred, axis=1), tf.argmax(y, axis=1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    n_batches = int(mnist.test.num_examples/batch_size)\n",
    "    total_correct_pred = 0\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "        accuracy_batch = sess.run(accuracy, feed_dict={x: X_batch, y:Y_batch, keep_prob: 1.})\n",
    "        total_correct_pred += accuracy_batch\n",
    "\n",
    "    print('Accuracy {0}'.format(total_correct_pred/mnist.test.num_examples))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw2_cnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
